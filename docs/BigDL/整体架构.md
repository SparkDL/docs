BigDL的代码架构模仿了Torch，神经网络的构建以`Module`为核心，具体运算则以`Tensor`为单位执行。

## 模型定义API
BigDL支持2套模型定义API

- Sequential
- Functional

对一个同样的例子：
```
Linear -> Sigmoid -> Softmax
```

采用Sequential的形式定义为：
```scala
val model = Sequential()
model.add(Linear(...))
model.add(Sigmoid())
model.add(Softmax())
```
Functional的形式为：
```scala
val linear = Linear(...).inputs()
val sigmoid = Sigmoid().inputs(linear)
val softmax = Softmax().inputs(sigmoid)
val model = Graph(Seq[linear], Seq[softmax])
```

后者的形式比较直观。

## Module
`Module`是BigDL中网络构建的基本单位，网络的每一种层都实现为一个`Module`。

`com.intel.analytics.bigdl.nn.abstractnn`包内定义了`AbstractModule`，它是所有`Module`的原始基类：
```scala
package com.intel.analytics.bigdl.nn.abstractnn
/**
 * Module is the basic component of a neural network. It forward activities and backward gradients.
 * Modules can connect to others to construct a complex neural network.
 *
 * @tparam A Input data type
 * @tparam B Output data type
 * @tparam T The numeric type in this module parameters.
 */
abstract class AbstractModule[A <: Activity: ClassTag, B <: Activity: ClassTag, T: ClassTag](
  implicit ev: TensorNumeric[T]) extends Serializable with InferShape
```


这是个泛型类，`Abstract[A,B,T]`，有3个参数`A,B,T`，`A`是输入的类型，`B`是输出的类型，都要求是`Activity`的子类，然后`T`是此Module使用参数的类型，可以是`Double`或者`Float`.

一个`Module`的核心功能肯定是前向传播进行推断和反向传播更新梯度, 分别对应`forward`和`backward`方法, 这两个方法是给出具体实现了的:
```scala
  /**
   * Takes an input object, and computes the corresponding output of the module. After a forward,
   * the output state variable should have been updated to the new value.
   *
   * @param input input data
   * @return output data
   */
  final def forward(input: A): B = {
    val before = System.nanoTime()
    try {
      updateOutput(input)
    } catch {
      case l: LayerException =>
        l.layerMsg = this.toString() + "/" + l.layerMsg
        throw l
      case e: Throwable =>
        throw new LayerException(this.toString(), e)
    }
    forwardTime += System.nanoTime() - before

    output
  }

  /**
   * Performs a back-propagation step through the module, with respect to the given input. In
   * general this method makes the assumption forward(input) has been called before, with the same
   * input. This is necessary for optimization reasons. If you do not respect this rule, backward()
   * will compute incorrect gradients.
   *
   * @param input input data
   * @param gradOutput gradient of next layer
   * @return gradient corresponding to input data
   */
  def backward(input: A, gradOutput: B): A = {
    val before = System.nanoTime()
    updateGradInput(input, gradOutput)
    accGradParameters(input, gradOutput)
    backwardTime += System.nanoTime() - before

    gradInput
  }
```

暂时可以只关注`forward`方法, 可以发现它就是调用了`updateOutput(input)`, 然后做一些统计工作. 

```scala
  /**
   * Computes the output using the current parameter set of the class and input. This function
   * returns the result which is stored in the output field.
   *
   * @param input
   * @return
   */
  def updateOutput(input: A): B
```