{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to SparkDL",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-sparkdl",
            "text": "",
            "title": "Welcome to SparkDL"
        },
        {
            "location": "/BigDL/\u6570\u636e\u8868\u793a/",
            "text": "\u6570\u636e\u8868\u793a\n\n\nBigDL\u7684\u5e95\u5c42\u7684\u6570\u636e\u8868\u793a\u5f62\u5f0f\u6709\n\n\n\n\nTensor\n\n\nSparseTensor\n\n\nTalbe\n\n\nSample\n\n\nMiniBatch\n\n\nDataSet\n\n\nOpenCVMat\n\n\nImageFeature\n\n\nImageFrame\n\n\n\n\n\u53ef\u4ee5\u53c2\u8003\u5b98\u65b9\u6587\u6863 \nData\n.\n\n\nTensor\n\n\nTensor\u662f\u6a21\u4effTorch\u800c\u6765, \u662f\u6700\u91cd\u8981\u7684\u6570\u636e\u7ed3\u6784.\n\n\n\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u591a\u7ef4\u7684\u77e9\u9635, \u5728\u5176\u4e4b\u4e0a\u5b9a\u4e49\u4e86\u5404\u79cd\u8fd0\u7b97. \u652f\u6301\nFloat\n\u6216\nDouble\n\u4e24\u79cd\u7c7b\u578b.\n\n\n\u4f7f\u7528\u7684\u65f6\u5019, \u53ef\u4ee5\u663e\u5f0f\u4f7f\u7528\u7c7b\u578b\u53c2\u6570:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nval tensor = Tensor[Float](2, 3)\n\n\n\n\n\u4e5f\u53ef\u4ee5\u5bfc\u5165\u4e00\u4e2a\u9690\u5f0f\u7c7b, \u9690\u5f0f\u5730\u8868\u660e\u4f60\u4f7f\u7528\u7684\u6570\u636e\u7c7b\u578b:\n\n\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nval tensor = Tensor(2, 3)\n\n\n\n\nTable\n\n\n\u540c\u6837\u6a21\u4effTorch, \u53ef\u4ee5\u770b\u4f5c\u952e\u503c\u5bf9map. \u4f7f\u7528\u8bed\u6cd5\u7cd6\nT\n\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2aTable\n\n\nScala example:\n\n\nimport com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nprintln(T(Tensor(2,2).fill(1), Tensor(2,2).fill(2)))\n\n\n\n\n\u8f93\u51fa\u662f: \n\n\n {\n    2: 2.0  2.0 \n       2.0  2.0 \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1: 1.0  1.0 \n       1.0  1.0 \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }\n\n\n\n\nSample\n\n\n\u4ee3\u8868\u4f60\u7528\u7684\u6570\u636e\u96c6, \u5f53\u7136\u5305\u62ec\nfeature\n\u548c\nlabel\n, \u8fd9\u91cc\u7684feature\u548clabel\u90fd\u662fTensor.\n\n\n\n\nThe case where feature is one tensor with a 1-element label.\n\n\n\n\nimport com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval image = Tensor(3, 32, 32).rand\nval label = 1f\nval sample = Sample(image, label)\n\n\n\n\n\n\nThe case where feature is a few tensors and label is also a few tensors.\n\n\n\n\nimport com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval features = Array(Tensor(2, 2).rand, Tensor(2, 2).rand)\nval labels = Array(Tensor(1).fill(1), Tensor(1).fill(-1))\nval sample = Sample(features, labels)",
            "title": "\u6570\u636e\u8868\u793a"
        },
        {
            "location": "/BigDL/\u6570\u636e\u8868\u793a/#_1",
            "text": "BigDL\u7684\u5e95\u5c42\u7684\u6570\u636e\u8868\u793a\u5f62\u5f0f\u6709   Tensor  SparseTensor  Talbe  Sample  MiniBatch  DataSet  OpenCVMat  ImageFeature  ImageFrame   \u53ef\u4ee5\u53c2\u8003\u5b98\u65b9\u6587\u6863  Data .",
            "title": "\u6570\u636e\u8868\u793a"
        },
        {
            "location": "/BigDL/\u6570\u636e\u8868\u793a/#tensor",
            "text": "Tensor\u662f\u6a21\u4effTorch\u800c\u6765, \u662f\u6700\u91cd\u8981\u7684\u6570\u636e\u7ed3\u6784.  \u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u591a\u7ef4\u7684\u77e9\u9635, \u5728\u5176\u4e4b\u4e0a\u5b9a\u4e49\u4e86\u5404\u79cd\u8fd0\u7b97. \u652f\u6301 Float \u6216 Double \u4e24\u79cd\u7c7b\u578b.  \u4f7f\u7528\u7684\u65f6\u5019, \u53ef\u4ee5\u663e\u5f0f\u4f7f\u7528\u7c7b\u578b\u53c2\u6570:  import com.intel.analytics.bigdl.tensor.Tensor\nval tensor = Tensor[Float](2, 3)  \u4e5f\u53ef\u4ee5\u5bfc\u5165\u4e00\u4e2a\u9690\u5f0f\u7c7b, \u9690\u5f0f\u5730\u8868\u660e\u4f60\u4f7f\u7528\u7684\u6570\u636e\u7c7b\u578b:  import com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nval tensor = Tensor(2, 3)",
            "title": "Tensor"
        },
        {
            "location": "/BigDL/\u6570\u636e\u8868\u793a/#table",
            "text": "\u540c\u6837\u6a21\u4effTorch, \u53ef\u4ee5\u770b\u4f5c\u952e\u503c\u5bf9map. \u4f7f\u7528\u8bed\u6cd5\u7cd6 T \u53ef\u4ee5\u521b\u5efa\u4e00\u4e2aTable  Scala example:  import com.intel.analytics.bigdl.utils.T\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\nprintln(T(Tensor(2,2).fill(1), Tensor(2,2).fill(2)))  \u8f93\u51fa\u662f:    {\n    2: 2.0  2.0 \n       2.0  2.0 \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n    1: 1.0  1.0 \n       1.0  1.0 \n       [com.intel.analytics.bigdl.tensor.DenseTensor$mcF$sp of size 2x2]\n }",
            "title": "Table"
        },
        {
            "location": "/BigDL/\u6570\u636e\u8868\u793a/#sample",
            "text": "\u4ee3\u8868\u4f60\u7528\u7684\u6570\u636e\u96c6, \u5f53\u7136\u5305\u62ec feature \u548c label , \u8fd9\u91cc\u7684feature\u548clabel\u90fd\u662fTensor.   The case where feature is one tensor with a 1-element label.   import com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval image = Tensor(3, 32, 32).rand\nval label = 1f\nval sample = Sample(image, label)   The case where feature is a few tensors and label is also a few tensors.   import com.intel.analytics.bigdl.dataset.Sample\nimport com.intel.analytics.bigdl.tensor.Tensor\nimport com.intel.analytics.bigdl.numeric.NumericFloat\n\nval features = Array(Tensor(2, 2).rand, Tensor(2, 2).rand)\nval labels = Array(Tensor(1).fill(1), Tensor(1).fill(-1))\nval sample = Sample(features, labels)",
            "title": "Sample"
        },
        {
            "location": "/BigDL/\u6a21\u578b\u5b9a\u4e49/",
            "text": "\u6a21\u578b\u5b9a\u4e49\n\n\nBigDL\u7684\u4ee3\u7801\u67b6\u6784\u6a21\u4eff\u4e86Torch\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u6784\u5efa\u4ee5\nModule\n\u4e3a\u6838\u5fc3\uff0c\u5177\u4f53\u8fd0\u7b97\u5219\u4ee5\nTensor\n\u4e3a\u5355\u4f4d\u6267\u884c\u3002\n\n\nBigDL\u652f\u63012\u5957\u6a21\u578b\u5b9a\u4e49API\n\n\n\n\nSequential\n\n\nFunctional\n\n\n\n\n\b\u5bf9\u4e00\u4e2a\u540c\u6837\u7684\u4f8b\u5b50\uff1a\n\n\nLinear -> Sigmoid -> Softmax\n\n\n\n\n\u91c7\u7528Sequential\u7684\u5f62\u5f0f\u5b9a\u4e49\u4e3a\uff1a\n\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())\n\n\n\n\nFunctional\u7684\u5f62\u5f0f\u4e3a\uff1a\n\n\nval linear = Linear(...).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = Softmax().inputs(sigmoid)\nval model = Graph(Seq[linear], Seq[softmax])\n\n\n\n\n\u540e\u8005\u7684\u5f62\u5f0f\u6bd4\u8f83\u76f4\u89c2\u3002\n\n\n\u5173\u4e8e\u6a21\u578b\u5b9a\u4e49\u7684\u66f4\u591a\u7ec6\u8282\u8bf7\u53c2\u8003BigDL\u7684\u6587\u6863(\u9700\u8981\u8ba4\u771f\u770b\u5b8c):\n\n\n\n\nSequential API\n\n\nFunctional API",
            "title": "\u6a21\u578b\u5b9a\u4e49"
        },
        {
            "location": "/BigDL/\u6a21\u578b\u5b9a\u4e49/#_1",
            "text": "BigDL\u7684\u4ee3\u7801\u67b6\u6784\u6a21\u4eff\u4e86Torch\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u6784\u5efa\u4ee5 Module \u4e3a\u6838\u5fc3\uff0c\u5177\u4f53\u8fd0\u7b97\u5219\u4ee5 Tensor \u4e3a\u5355\u4f4d\u6267\u884c\u3002  BigDL\u652f\u63012\u5957\u6a21\u578b\u5b9a\u4e49API   Sequential  Functional   \b\u5bf9\u4e00\u4e2a\u540c\u6837\u7684\u4f8b\u5b50\uff1a  Linear -> Sigmoid -> Softmax  \u91c7\u7528Sequential\u7684\u5f62\u5f0f\u5b9a\u4e49\u4e3a\uff1a  val model = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())  Functional\u7684\u5f62\u5f0f\u4e3a\uff1a  val linear = Linear(...).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = Softmax().inputs(sigmoid)\nval model = Graph(Seq[linear], Seq[softmax])  \u540e\u8005\u7684\u5f62\u5f0f\u6bd4\u8f83\u76f4\u89c2\u3002  \u5173\u4e8e\u6a21\u578b\u5b9a\u4e49\u7684\u66f4\u591a\u7ec6\u8282\u8bf7\u53c2\u8003BigDL\u7684\u6587\u6863(\u9700\u8981\u8ba4\u771f\u770b\u5b8c):   Sequential API  Functional API",
            "title": "\u6a21\u578b\u5b9a\u4e49"
        },
        {
            "location": "/BigDL/Module/",
            "text": "Module\n\n\nModule\n\u662fBigDL\u4e2d\b\u7f51\u7edc\u6784\u5efa\u7684\u57fa\u672c\u5355\u4f4d\uff0c\u7f51\u7edc\u7684\u6bcf\u4e00\u79cd\u5c42\u90fd\u5b9e\u73b0\u4e3a\u4e00\u4e2a\nModule\n.\n\n\n\u9996\u5148\u6765\u770b\u4e00\u4e0bModule\u7684\u7ee7\u627f\u4f53\u7cfb.\n\n\n\n\n\u8bf4\u660e\u4e00\u4e0b, \u6700\u5e95\u5c42\u7684\u662f\nAbstractModule\n\u62bd\u8c61\u7c7b, \u8fd9\u662f\u6240\u6709\nModule\n\u7684\u57fa\u7c7b.\n\n\n\u7136\u540e\u662f\u5b83\u7684\u4e00\u4e2a\u5b50\u7c7b\nTensorModule\n, \u8fd9\u4e5f\u662f\u4e00\u4e2a\u62bd\u8c61\u7c7b, \u5927\u90e8\u5206\u5c42\u90fd\u662f\u7ee7\u627f\u8fd9\u4e2a\u7c7b, \u6bd4\u5982\u5168\u8fde\u63a5, \u5377\u79ef\u7b49.\n\n\n\u53e6\u5916\u5b83\u8fd8\u6709\u51e0\u4e2a\u5b50\u7c7b\n\n\n\n\nContainer\n Module\u7684\u5bb9\u5668, \u53ef\u4ee5\u7406\u89e3\u4e3a\u628a\u591a\u4e2aModule\u6253\u5305\u6210\u4e00\u4e2a\n\n\nCell\n \u4e3a\u5faa\u73af\u795e\u7ecf\u5143\u8bbe\u8ba1\u7684, \u6bd4\u5982RNNCell, LSTMCell\u90fd\u7ee7\u627f\u81ea\u8fd9\u4e2a\u7c7b\n\n\nCAddTable\n, \nCSubTable\n... \u8fd9\u662f\u5176\u4ed6\u7684\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7684\u6bd4\u8f83\u7279\u6b8a\u7684Module\n\n\n\n\nContainer, Sequenstial, Graph\n\n\n\u8bf7\u53c2\u8003 \nContainers\n\n\n\u6ce8\u610f\u8fd9\u91cc\u6709\u4e24\u4e2a\u6bd4\u8f83\u7279\u6b8a\u7684\u5bb9\u5668, \nSequenstial\n\u548c\nGraph\n, \u8fd9\u662f\u6211\u4eec\u6784\u5efa\u6a21\u578b\u7684\u9876\u5c42API.\n\n\n\u518d\u6765\u56de\u987e\u4e00\u4e0b\n\u6a21\u578b\u5b9a\u4e49\n\u7684\u4e24\u79cd\u65b9\u5f0f.\n\n\n\u91c7\u7528Sequential\u7684\u5f62\u5f0f\u5b9a\u4e49\u4e3a\uff1a\n\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())\n\n\n\n\nFunctional\u7684\u5f62\u5f0f\u4e3a\uff1a\n\n\nval linear = Linear(...).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = Softmax().inputs(sigmoid)\nval model = Graph(Seq[linear], Seq[softmax])\n\n\n\n\n\u524d\u8005\u662f\u5411\nSequential\n\u5bb9\u5668\u5185\u4e0d\u65ad\u6dfb\u52a0\u65b0\u7684\u5c42, \u540e\u8005\u5219\u662f\u4f7f\u7528\u5404\u6a21\u5757\u672c\u8eab\u7684\ninputs\n\u65b9\u6cd5\u8fde\u63a5\u8d77\u6765, \u6700\u540e\u4f7f\u7528\u8f93\u5165\u8282\u70b9\u548c\u8f93\u51fa\u8282\u70b9\u6784\u5efa\u4e00\u4e2a\nGraph\n.\n\n\nAbstractModule\n\n\ncom.intel.analytics.bigdl.nn.abstractnn\n\u5305\u5185\u5b9a\u4e49\u4e86\nAbstractModule\n\uff0c\u5b83\u662f\u6240\u6709\nModule\n\u7684\u539f\u59cb\u57fa\u7c7b\uff1a\n\n\npackage com.intel.analytics.bigdl.nn.abstractnn\n/**\n * Module is the basic component of a neural network. It forward activities and backward gradients.\n * Modules can connect to others to construct a complex neural network.\n *\n * @tparam A Input data type\n * @tparam B Output data type\n * @tparam T The numeric type in this module parameters.\n */\nabstract class AbstractModule[A <: Activity: ClassTag, B <: Activity: ClassTag, T: ClassTag](\n  implicit ev: TensorNumeric[T]) extends Serializable with InferShape\n\n\n\n\n\u8fd9\u662f\u4e2a\u6cdb\u578b\u7c7b\uff0c\nAbstract[A,B,T]\n\uff0c\u67093\u4e2a\u53c2\u6570\nA,B,T\n\uff0c\nA\n\u662f\u8f93\u5165\u7684\u7c7b\u578b\uff0c\nB\n\u662f\u8f93\u51fa\u7684\u7c7b\u578b\uff0c\u90fd\u8981\u6c42\u662f\nActivity\n\u7684\u5b50\u7c7b\uff0c\u7136\u540e\nT\n\u662f\u6b64Module\u4f7f\u7528\u53c2\u6570\u7684\u7c7b\u578b\uff0c\u53ef\u4ee5\u662f\nDouble\n\u6216\u8005\nFloat\n.\n\n\n\u4e00\u4e2a\nModule\n\u7684\u6838\u5fc3\u529f\u80fd\u80af\u5b9a\u662f\u524d\u5411\u4f20\u64ad\u8fdb\u884c\u63a8\u65ad\u548c\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u68af\u5ea6, \u5206\u522b\u5bf9\u5e94\nforward\n\u548c\nbackward\n\u65b9\u6cd5, \u8fd9\u4e24\u4e2a\u65b9\u6cd5\u662f\u7ed9\u51fa\u5177\u4f53\u5b9e\u73b0\u4e86\u7684:\n\n\n  /**\n   * Takes an input object, and computes the corresponding output of the module. After a forward,\n   * the output state variable should have been updated to the new value.\n   *\n   * @param input input data\n   * @return output data\n   */\n  final def forward(input: A): B = {\n    val before = System.nanoTime()\n    try {\n      updateOutput(input)\n    } catch {\n      case l: LayerException =>\n        l.layerMsg = this.toString() + \"/\" + l.layerMsg\n        throw l\n      case e: Throwable =>\n        throw new LayerException(this.toString(), e)\n    }\n    forwardTime += System.nanoTime() - before\n\n    output\n  }\n\n  /**\n   * Performs a back-propagation step through the module, with respect to the given input. In\n   * general this method makes the assumption forward(input) has been called before, with the same\n   * input. This is necessary for optimization reasons. If you do not respect this rule, backward()\n   * will compute incorrect gradients.\n   *\n   * @param input input data\n   * @param gradOutput gradient of next layer\n   * @return gradient corresponding to input data\n   */\n  def backward(input: A, gradOutput: B): A = {\n    val before = System.nanoTime()\n    updateGradInput(input, gradOutput)\n    accGradParameters(input, gradOutput)\n    backwardTime += System.nanoTime() - before\n\n    gradInput\n  }\n\n\n\n\n\u6682\u65f6\u53ef\u4ee5\u53ea\u5173\u6ce8\nforward\n\u65b9\u6cd5, \u53ef\u4ee5\u53d1\u73b0\u5b83\u5c31\u662f\u8c03\u7528\u4e86\nupdateOutput(input)\n,\b \u7136\u540e\u505a\u4e00\u4e9b\u7edf\u8ba1\u5de5\u4f5c. \n\n\n  /**\n   * Computes the output using the current parameter set of the class and input. This function\n   * returns the result which is stored in the output field.\n   *\n   * @param input\n   * @return\n   */\n  def updateOutput(input: A): B\n\n\n\n\n\u8fd9\u662f\u4e2a\u62bd\u8c61\u65b9\u6cd5, \u7559\u7ed9\u5b50\u7c7b\u53bb\u5b9e\u73b0. \u6211\u4eec\u7ee7\u627f\u8fd9\u4e2a\u7c7b\u7136\u540e\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u5373\u53ef\u5b9e\u73b0\u524d\u5411\u4f20\u64ad\u8fdb\u884cinference\u4e86.\n\n\nTensorModule\n\n\n\b\u4e0a\u9762\u5b9a\u4e49\u4e86Module\u7684\u62bd\u8c61\u7c7b, \u7136\u540eBigDL\b\u5177\u4f53\u4f7f\u7528\u7684\u662f\u5b83\u7684\u4e00\u4e2a\u5b50\u7c7b\nTensorModule[T]\n, \u5206\u522b\u5c06\nAbstractModule\n\u7684\u4e09\u4e2a\u53c2\u6570\u7c7b\u578b\u8bbe\u4e3a\nTensor[T], Tensor[T], T\n, \u4e5f\u5c31\u662f\u8f93\u5165\u8f93\u51fa\u90fd\u662f\nTensor\n\u7c7b\u578b, \u8fd9\u6837\u5c31\u53ef\u4ee5\u628a\u5177\u4f53\u7684\u8ba1\u7b97\u8fc7\u7a0b\u5168\u90e8\u4f7f\u7528\nTensor\n\u7684\u8fd0\u7b97\u5b9e\u73b0.\n\n\n/**\n * [[TensorModule]] is an abstract sub-class of [[AbstractModule]], whose\n * input and output type both are [[Tensor]].\n *\n * @tparam T The numeric type in this module parameters\n */\nabstract class TensorModule[T: ClassTag]\n  (implicit ev: TensorNumeric[T]) extends AbstractModule[Tensor[T], Tensor[T], T]\n\n\n\n\n\u4f8b\u5b50\n\n\n\u770b\u4e00\u4e2a\u6700\u7b80\u5355\u7684\u4f8b\u5b50, \nAdd\n\u5c42, \u5b83\u7b80\u5355\u7684\u5c06\u6bcf\u4e2a\u8f93\u5165\u5404\u52a0\u4e0a\u4e00\u4e2a\u503c: \n\n\n/**\n * adds a bias term to input data ;\n *\n * @param inputSize size of input data\n */\n@SerialVersionUID(4268487849759172896L)\nclass Add[T: ClassTag](val inputSize: Int\n  )(implicit ev: TensorNumeric[T]) extends TensorModule[T] with Initializable {\n\n  val bias = Tensor[T](inputSize)\n\n  override def updateOutput(input: Tensor[T]): Tensor[T] = {\n    output.resizeAs(input).copy(input)\n    if (input.isSameSizeAs(bias)) {\n      output.add(bias)\n    } else {\n      val batchSize = input.size(1)\n      ones.resize(batchSize).fill(ev.one)\n      val biasLocal = bias.view(bias.size.product)\n      val outputLocal = output.view(batchSize, output.size.product/batchSize)\n      outputLocal.addr(ev.fromType[Int](1), ones, biasLocal)\n    }\n    output\n  }\n\n\n\n\noutput\n\u548c\nbias\n\u90fd\u662f\u4e00\u4e2a\nTensor\n\u7c7b\u7684\u5bf9\u8c61, \u5982\u679c\u4e8c\u8005\u7684\u5c3a\u5bf8\u76f8\u540c\u7684\u8bdd, \u76f4\u63a5\u8c03\u7528Tensor\u7684add\u65b9\u6cd5\n\n\noutput.add(bias)\n\n\n\n\n\u5c31\u53ef\u4ee5\u5f97\u5230\u8f93\u51fa.",
            "title": "Module"
        },
        {
            "location": "/BigDL/Module/#module",
            "text": "Module \u662fBigDL\u4e2d\b\u7f51\u7edc\u6784\u5efa\u7684\u57fa\u672c\u5355\u4f4d\uff0c\u7f51\u7edc\u7684\u6bcf\u4e00\u79cd\u5c42\u90fd\u5b9e\u73b0\u4e3a\u4e00\u4e2a Module .  \u9996\u5148\u6765\u770b\u4e00\u4e0bModule\u7684\u7ee7\u627f\u4f53\u7cfb.   \u8bf4\u660e\u4e00\u4e0b, \u6700\u5e95\u5c42\u7684\u662f AbstractModule \u62bd\u8c61\u7c7b, \u8fd9\u662f\u6240\u6709 Module \u7684\u57fa\u7c7b.  \u7136\u540e\u662f\u5b83\u7684\u4e00\u4e2a\u5b50\u7c7b TensorModule , \u8fd9\u4e5f\u662f\u4e00\u4e2a\u62bd\u8c61\u7c7b, \u5927\u90e8\u5206\u5c42\u90fd\u662f\u7ee7\u627f\u8fd9\u4e2a\u7c7b, \u6bd4\u5982\u5168\u8fde\u63a5, \u5377\u79ef\u7b49.  \u53e6\u5916\u5b83\u8fd8\u6709\u51e0\u4e2a\u5b50\u7c7b   Container  Module\u7684\u5bb9\u5668, \u53ef\u4ee5\u7406\u89e3\u4e3a\u628a\u591a\u4e2aModule\u6253\u5305\u6210\u4e00\u4e2a  Cell  \u4e3a\u5faa\u73af\u795e\u7ecf\u5143\u8bbe\u8ba1\u7684, \u6bd4\u5982RNNCell, LSTMCell\u90fd\u7ee7\u627f\u81ea\u8fd9\u4e2a\u7c7b  CAddTable ,  CSubTable ... \u8fd9\u662f\u5176\u4ed6\u7684\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7684\u6bd4\u8f83\u7279\u6b8a\u7684Module",
            "title": "Module"
        },
        {
            "location": "/BigDL/Module/#container-sequenstial-graph",
            "text": "\u8bf7\u53c2\u8003  Containers  \u6ce8\u610f\u8fd9\u91cc\u6709\u4e24\u4e2a\u6bd4\u8f83\u7279\u6b8a\u7684\u5bb9\u5668,  Sequenstial \u548c Graph , \u8fd9\u662f\u6211\u4eec\u6784\u5efa\u6a21\u578b\u7684\u9876\u5c42API.  \u518d\u6765\u56de\u987e\u4e00\u4e0b \u6a21\u578b\u5b9a\u4e49 \u7684\u4e24\u79cd\u65b9\u5f0f.  \u91c7\u7528Sequential\u7684\u5f62\u5f0f\u5b9a\u4e49\u4e3a\uff1a  val model = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())  Functional\u7684\u5f62\u5f0f\u4e3a\uff1a  val linear = Linear(...).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = Softmax().inputs(sigmoid)\nval model = Graph(Seq[linear], Seq[softmax])  \u524d\u8005\u662f\u5411 Sequential \u5bb9\u5668\u5185\u4e0d\u65ad\u6dfb\u52a0\u65b0\u7684\u5c42, \u540e\u8005\u5219\u662f\u4f7f\u7528\u5404\u6a21\u5757\u672c\u8eab\u7684 inputs \u65b9\u6cd5\u8fde\u63a5\u8d77\u6765, \u6700\u540e\u4f7f\u7528\u8f93\u5165\u8282\u70b9\u548c\u8f93\u51fa\u8282\u70b9\u6784\u5efa\u4e00\u4e2a Graph .",
            "title": "Container, Sequenstial, Graph"
        },
        {
            "location": "/BigDL/Module/#abstractmodule",
            "text": "com.intel.analytics.bigdl.nn.abstractnn \u5305\u5185\u5b9a\u4e49\u4e86 AbstractModule \uff0c\u5b83\u662f\u6240\u6709 Module \u7684\u539f\u59cb\u57fa\u7c7b\uff1a  package com.intel.analytics.bigdl.nn.abstractnn\n/**\n * Module is the basic component of a neural network. It forward activities and backward gradients.\n * Modules can connect to others to construct a complex neural network.\n *\n * @tparam A Input data type\n * @tparam B Output data type\n * @tparam T The numeric type in this module parameters.\n */\nabstract class AbstractModule[A <: Activity: ClassTag, B <: Activity: ClassTag, T: ClassTag](\n  implicit ev: TensorNumeric[T]) extends Serializable with InferShape  \u8fd9\u662f\u4e2a\u6cdb\u578b\u7c7b\uff0c Abstract[A,B,T] \uff0c\u67093\u4e2a\u53c2\u6570 A,B,T \uff0c A \u662f\u8f93\u5165\u7684\u7c7b\u578b\uff0c B \u662f\u8f93\u51fa\u7684\u7c7b\u578b\uff0c\u90fd\u8981\u6c42\u662f Activity \u7684\u5b50\u7c7b\uff0c\u7136\u540e T \u662f\u6b64Module\u4f7f\u7528\u53c2\u6570\u7684\u7c7b\u578b\uff0c\u53ef\u4ee5\u662f Double \u6216\u8005 Float .  \u4e00\u4e2a Module \u7684\u6838\u5fc3\u529f\u80fd\u80af\u5b9a\u662f\u524d\u5411\u4f20\u64ad\u8fdb\u884c\u63a8\u65ad\u548c\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u68af\u5ea6, \u5206\u522b\u5bf9\u5e94 forward \u548c backward \u65b9\u6cd5, \u8fd9\u4e24\u4e2a\u65b9\u6cd5\u662f\u7ed9\u51fa\u5177\u4f53\u5b9e\u73b0\u4e86\u7684:    /**\n   * Takes an input object, and computes the corresponding output of the module. After a forward,\n   * the output state variable should have been updated to the new value.\n   *\n   * @param input input data\n   * @return output data\n   */\n  final def forward(input: A): B = {\n    val before = System.nanoTime()\n    try {\n      updateOutput(input)\n    } catch {\n      case l: LayerException =>\n        l.layerMsg = this.toString() + \"/\" + l.layerMsg\n        throw l\n      case e: Throwable =>\n        throw new LayerException(this.toString(), e)\n    }\n    forwardTime += System.nanoTime() - before\n\n    output\n  }\n\n  /**\n   * Performs a back-propagation step through the module, with respect to the given input. In\n   * general this method makes the assumption forward(input) has been called before, with the same\n   * input. This is necessary for optimization reasons. If you do not respect this rule, backward()\n   * will compute incorrect gradients.\n   *\n   * @param input input data\n   * @param gradOutput gradient of next layer\n   * @return gradient corresponding to input data\n   */\n  def backward(input: A, gradOutput: B): A = {\n    val before = System.nanoTime()\n    updateGradInput(input, gradOutput)\n    accGradParameters(input, gradOutput)\n    backwardTime += System.nanoTime() - before\n\n    gradInput\n  }  \u6682\u65f6\u53ef\u4ee5\u53ea\u5173\u6ce8 forward \u65b9\u6cd5, \u53ef\u4ee5\u53d1\u73b0\u5b83\u5c31\u662f\u8c03\u7528\u4e86 updateOutput(input) ,\b \u7136\u540e\u505a\u4e00\u4e9b\u7edf\u8ba1\u5de5\u4f5c.     /**\n   * Computes the output using the current parameter set of the class and input. This function\n   * returns the result which is stored in the output field.\n   *\n   * @param input\n   * @return\n   */\n  def updateOutput(input: A): B  \u8fd9\u662f\u4e2a\u62bd\u8c61\u65b9\u6cd5, \u7559\u7ed9\u5b50\u7c7b\u53bb\u5b9e\u73b0. \u6211\u4eec\u7ee7\u627f\u8fd9\u4e2a\u7c7b\u7136\u540e\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u5373\u53ef\u5b9e\u73b0\u524d\u5411\u4f20\u64ad\u8fdb\u884cinference\u4e86.",
            "title": "AbstractModule"
        },
        {
            "location": "/BigDL/Module/#tensormodule",
            "text": "\b\u4e0a\u9762\u5b9a\u4e49\u4e86Module\u7684\u62bd\u8c61\u7c7b, \u7136\u540eBigDL\b\u5177\u4f53\u4f7f\u7528\u7684\u662f\u5b83\u7684\u4e00\u4e2a\u5b50\u7c7b TensorModule[T] , \u5206\u522b\u5c06 AbstractModule \u7684\u4e09\u4e2a\u53c2\u6570\u7c7b\u578b\u8bbe\u4e3a Tensor[T], Tensor[T], T , \u4e5f\u5c31\u662f\u8f93\u5165\u8f93\u51fa\u90fd\u662f Tensor \u7c7b\u578b, \u8fd9\u6837\u5c31\u53ef\u4ee5\u628a\u5177\u4f53\u7684\u8ba1\u7b97\u8fc7\u7a0b\u5168\u90e8\u4f7f\u7528 Tensor \u7684\u8fd0\u7b97\u5b9e\u73b0.  /**\n * [[TensorModule]] is an abstract sub-class of [[AbstractModule]], whose\n * input and output type both are [[Tensor]].\n *\n * @tparam T The numeric type in this module parameters\n */\nabstract class TensorModule[T: ClassTag]\n  (implicit ev: TensorNumeric[T]) extends AbstractModule[Tensor[T], Tensor[T], T]",
            "title": "TensorModule"
        },
        {
            "location": "/BigDL/Module/#_1",
            "text": "\u770b\u4e00\u4e2a\u6700\u7b80\u5355\u7684\u4f8b\u5b50,  Add \u5c42, \u5b83\u7b80\u5355\u7684\u5c06\u6bcf\u4e2a\u8f93\u5165\u5404\u52a0\u4e0a\u4e00\u4e2a\u503c:   /**\n * adds a bias term to input data ;\n *\n * @param inputSize size of input data\n */\n@SerialVersionUID(4268487849759172896L)\nclass Add[T: ClassTag](val inputSize: Int\n  )(implicit ev: TensorNumeric[T]) extends TensorModule[T] with Initializable {\n\n  val bias = Tensor[T](inputSize)\n\n  override def updateOutput(input: Tensor[T]): Tensor[T] = {\n    output.resizeAs(input).copy(input)\n    if (input.isSameSizeAs(bias)) {\n      output.add(bias)\n    } else {\n      val batchSize = input.size(1)\n      ones.resize(batchSize).fill(ev.one)\n      val biasLocal = bias.view(bias.size.product)\n      val outputLocal = output.view(batchSize, output.size.product/batchSize)\n      outputLocal.addr(ev.fromType[Int](1), ones, biasLocal)\n    }\n    output\n  }  output \u548c bias \u90fd\u662f\u4e00\u4e2a Tensor \u7c7b\u7684\u5bf9\u8c61, \u5982\u679c\u4e8c\u8005\u7684\u5c3a\u5bf8\u76f8\u540c\u7684\u8bdd, \u76f4\u63a5\u8c03\u7528Tensor\u7684add\u65b9\u6cd5  output.add(bias)  \u5c31\u53ef\u4ee5\u5f97\u5230\u8f93\u51fa.",
            "title": "\u4f8b\u5b50"
        },
        {
            "location": "/BigDL/Transformer/",
            "text": "Transformer.md",
            "title": "Transformer.md"
        },
        {
            "location": "/BigDL/Transformer/#transformermd",
            "text": "",
            "title": "Transformer.md"
        },
        {
            "location": "/BigDL/inference\u8c03\u7528\u94fe\u5206\u6790/",
            "text": "inference\u8c03\u7528\u94fe\u5206\u6790\n\n\n\u4ee5\u6700\u7b80\u5355\u7684lenet5\u4e3a\u4f8b, \u63a2\u7a76inference\u8fc7\u7a0b\u7684\u8c03\u7528\u94fe\n\n\n\u793a\u4f8b\u4ee3\u7801\u4f4d\u4e8e'spark/dl/src/main/scala/com/pzque/sparkdl/lenet', \u6a21\u578b\u7684checkpoint\u5df2\u4fdd\u5b58\u597d, \u4e0b\u8f7d\u597d\u6570\u636e\u540e\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c'Test.scala'\u67e5\u770b\u6d4b\u8bd5\u96c6\u4e0a\u7684\u63a8\u65ad\u51c6\u786e\u7387.\n\n\nlenet\u6a21\u578b\u5b9a\u4e49\n\n\n\u9996\u5148\u770b\u4e00\u4e0blenet\u6a21\u578b\u7684\u5b9a\u4e49.\n\n\napply\n\u548c\ngraph\n\u51fd\u6570\u5206\u522b\u4f7f\u7528\u4e86Sequential\u548cGraph\u7684API\u5b9a\u4e49\u6a21\u578b, \u4e8c\u8005\u662f\u7b49\u4ef7\u7684.\n\n\n\u6a21\u578b\u7684\u7ed3\u6784\u975e\u5e38\u7b80\u5355, \u5728\u6d4b\u8bd5\u96c6\u4e0a\u53ef\u4ee5\u8fbe\u523098.93%\u7684\u51c6\u786e\u7387.\n\n\n28*28 -> (Conv -> MaxPooling)*2 -> (FullConnected)*2 -> LogSoftMax\n\n\n\n\nobject LeNet5 {\n  def apply(classNum: Int): Module[Float] = {\n    val model = Sequential()\n    model.add(Reshape(Array(1, 28, 28)))\n      .add(SpatialConvolution(1, 6, 5, 5).setName(\"conv1_5x5\"))\n      .add(Tanh())\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Tanh())\n      .add(SpatialConvolution(6, 12, 5, 5).setName(\"conv2_5x5\"))\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Reshape(Array(12 * 4 * 4)))\n      .add(Linear(12 * 4 * 4, 100).setName(\"fc1\"))\n      .add(Tanh())\n      .add(Linear(100, classNum).setName(\"fc2\"))\n      .add(LogSoftMax())\n  }\n  def graph(classNum: Int): Module[Float] = {\n    val input = Reshape(Array(1, 28, 28)).inputs()\n    val conv1 = SpatialConvolution(1, 6, 5, 5).setName(\"conv1_5x5\").inputs(input)\n    val tanh1 = Tanh().inputs(conv1)\n    val pool1 = SpatialMaxPooling(2, 2, 2, 2).inputs(tanh1)\n    val tanh2 = Tanh().inputs(pool1)\n    val conv2 = SpatialConvolution(6, 12, 5, 5).setName(\"conv2_5x5\").inputs(tanh2)\n    val pool2 = SpatialMaxPooling(2, 2, 2, 2).inputs(conv2)\n    val reshape = Reshape(Array(12 * 4 * 4)).inputs(pool2)\n    val fc1 = Linear(12 * 4 * 4, 100).setName(\"fc1\").inputs(reshape)\n    val tanh3 = Tanh().inputs(fc1)\n    val fc2 = Linear(100, classNum).setName(\"fc2\").inputs(tanh3)\n    val output = LogSoftMax().inputs(fc2)\n\n    Graph(input, output)\n  }\n}\n\n\n\n\ninference\u8c03\u7528\u94fe\n\n\ninfrence\u7684\u6838\u5fc3\u4ee3\u7801\u5982\u4e0b: \n\n\n      // \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e, \u8c03\u7528SparkContext\u7c7b\u7684parallize\u65b9\u6cd5\u5c06\u5176\u8f6c\u4e3aRDD\n      val rddData: RDD[ByteRecord] = sc.parallelize(load(validationData, validationLabel), partitionNum)\n\n      // \u5b9a\u4e49\u4e00\u4e2a\u6570\u636e\u9884\u5904\u7406\u5668, \u5c06ByteRecord\u683c\u5f0f\u8f6c\u4e3aSample[Float]\n      val transformer: Transformer[ByteRecord, Sample[Float]] =\n        BytesToGreyImg(28, 28) -> GreyImgNormalizer(testMean, testStd) -> GreyImgToSample()\n\n      // \u4f7f\u7528transformer\u6784\u9020\u9a8c\u8bc1\u96c6RDD\n      val evaluationSet: RDD[Sample[Float]] = transformer(rddData)\n\n      // \u52a0\u8f7d\u6a21\u578b\n      val model = Module.load[Float](param.model)\n\n      // \u6267\u884c\u6a21\u578b, \u83b7\u53d6\u7ed3\u679c\n      val result = model.evaluate(evaluationSet,\n        Array(new Top1Accuracy[Float]), Some(param.batchSize))\n\n\n\n\n\u524d\u9762\u7684\u4e00\u5806\u90fd\u662f\u4f7f\u7528spark\u7684RDD\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\u4e0e\u8f6c\u6362, \u6700\u540e\u5f97\u5230\nevaluationSet\n, \u4e5f\u662f\u4e00\u4e2aRDD, \u5143\u7d20\u662f\nSample[Flaot]\n\u7684\u7c7b\u578b.\n\n\n\u6211\u4eec\u770b\u5230\u5176\u5148\u662f\u901a\u8fc7\nModule.load[Float]\n\u5c06\u6a21\u578b\u52a0\u8f7d\u8fdb\u6765, \u7136\u540e\u5229\u7528\u6a21\u578b\u6267\u884cevaluate\u64cd\u4f5c.\n\n\n\u6211\u4eec\u9700\u8981\u5173\u6ce8\u8fd9\u4e00\u53e5:\n\n\nmodel.evaluate(evaluationSet,\n        Array(new Top1Accuracy[Float]), \n        Some(param.batchSize))\n\n\n\n\n\u627e\u5230\u5b83\u7684\u5b9a\u4e49, \u4f4d\u4e8e\nAbstractModule\n\u7c7b:\n\n\n  /**\n   * use ValidationMethod to evaluate module on the given rdd dataset\n   * @param dataset dataset for test\n   * @param vMethods validation methods\n   * @param batchSize total batchsize of all partitions,\n   *                  optional param and default 4 * partitionNum of dataset\n   * @return\n   */\n  final def evaluate(\n    dataset: RDD[Sample[T]],\n    vMethods: Array[ValidationMethod[T]],\n    batchSize: Option[Int] = None\n  ): Array[(ValidationResult, ValidationMethod[T])] = {\n    Evaluator(this).test(dataset, vMethods, batchSize)\n  }\n\n\n\n\n\u4e09\u4e2a\u53c2\u6570,\n\n\n\n\ndataset\n: \u662f\u4f60\u8981\u8fd0\u884c\u6a21\u578b\u7684\u6570\u636e\u96c6\n\n\nvMethods\n: \u662f\u6700\u540e\u6a21\u578b\u8fd0\u884c\u5b8c\u6210\u8fd0\u884c\u7684\u4e00\u4e9b\u7edf\u8ba1\u5de5\u4f5c, \u6bd4\u5982\u8fd9\u91cc\u7684Top1Accuracy\u5c31\u662f\u7edf\u8ba1\u4e00\u4e0b\u51c6\u786e\u7387 \n\n\nbatchSize\n: \u6ce8\u610f\u8fd9\u4e2a\u4e0d\u662f\u673a\u5668\u5b66\u4e60\u7684\u90a3\u4e2abatchsize(\u6bcf\u4e2abatch\u7684\u5927\u5c0f), \u800c\u662f\u5c06\u5168\u90e8\u7684\u6570\u636e\u96c6\u5206\u6210\u591a\u5c11batch\n\n\n\n\n\u7136\u540e\u6700\u540e\u6267\u884c\u6a21\u578b\u7684\u4ee3\u7801\u5c31\u662f\nEvaluator(this).test(dataset, vMethods, batchSize)\n\u4e86, \u4e0b\u9762\u6765\u770b\u4e00\u4e0b\u5b83\u7684\u5b9e\u73b0.\n\n\nEvaluator\n\n\n```scala\n/*\n\n * model evaluator\n * @param model model to be evaluated\n \n/\nclass Evaluator[T: ClassTag] private\noptim\n(implicit ev: TensorNumeric[T])\n  extends Serializable {\n\n\nprivate val batchPerPartition = 4\n\n\n/*\n\n   * Applies ValidationMethod to the model and rdd dataset.\n   * @param vMethods\n   * @param batchSize total batchsize\n   * @return\n   \n/\n  def test(dataset: RDD[Sample[T]],\n   vMethods: Array[ValidationMethod[T]],\n   batchSize: Option[Int] = None): Array[(ValidationResult, ValidationMethod[T])] = {\n\n\nval modelBroad = ModelBroadcast[T]().broadcast(dataset.sparkContext, model.evaluate())\nval partitionNum = dataset.partitions.length\n\nval totalBatch = batchSize.getOrElse(batchPerPartition * partitionNum)\nval otherBroad = dataset.sparkContext.broadcast(vMethods, SampleToMiniBatch(\n  batchSize = totalBatch, partitionNum = Some(partitionNum)))\n\ndataset.mapPartitions(partition => {\n  val localModel = modelBroad.value()\n  val localMethod = otherBroad.value._1.map(_.clone())\n  val localTransformer = otherBroad.value._2.cloneTransformer()\n  val miniBatch = localTransformer(partition)\n  miniBatch.map(batch => {\n    val output = localModel.forward(batch.getInput())\n    localMethod.map(validation => {\n      validation(output, batch.getTarget())\n    })\n  })\n}).reduce((left, right) => {\n    left.zip(right).map { case (l, r) => l + r }\n}).zip(vMethods)\n\n\n\n}\n}\n\n\n\n\u4e0a\u9762\u662f\u8fd9\u4e2a\u7c7b\u7684\u5168\u90e8\u4ee3\u7801, \u8fd9\u4e2a\u7c7b\u4e5f\u53ea\u662f\u5728\u5168\u5c40\u505a\u8c03\u5ea6, \u5f88\u7b80\u5355. \u5177\u4f53\u7684\u6267\u884c\u903b\u8f91\u5f53\u7136\u8fd8\u662f\u5728`AbstractModule`\u7684\u5b9e\u73b0\u7c7b\u91cc\u5b9a\u4e49.\n\n\u5982\u4ee3\u7801\u6240\u793a, \u5728\u4e00\u4e2aRDD\u6570\u636e\u96c6\u4e0a\u6267\u884c\u6a21\u578b\u6709\u5982\u4e0b\u51e0\u6b65:\n\n**1.\u5c06\u6a21\u578b\u5e7f\u64ad\u5230\u5404\u4e2a\u8282\u70b9**\n\n```scala\nval modelBroad = ModelBroadcast[T]().broadcast(dataset.sparkContext, model.evaluate())\n\n\n\n\n\u8fd9\u4e00\u53e5\u5c06\u6a21\u578b\u62f7\u8d1d\u5230\u4e86\u6bcf\u4e00\u4e2aspark\u8282\u70b9\u4e0a, \u8ba9\u5176\u90fd\u80fd\u8bbf\u95ee\u5230.\n\n\n2.\u5c06vMethods\u548c\u4e00\u4e2a\u80fd\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4e00\u4e2a\u4e2abatch\u7684transformer\u5e7f\u64ad\u5230\u5404\u4e2a\u8282\u70b9\n\n\nval otherBroad = dataset.sparkContext.broadcast\n(\n vMethods, \n SampleToMiniBatch(batchSize = totalBatch, partitionNum = Some(partitionNum))\n )\n\n\n\n\n\u8fd9\u91cc\u6ce8\u610f\u4e00\u4e0b\u4e00\u4e2ascala\u8bed\u6cd5\u7684\u5751, \u4e8b\u5b9e\u4e0a\nbroadcast\n\u51fd\u6570\u53ea\u80fd\u63a5\u53d7\u4e00\u4e2a\u53c2\u6570, \u4f46\u662fscala\u652f\u6301\u51fd\u6570\u4e0d\u5e26\u62ec\u53f7\u7684\u8c03\u7528\u8bed\u6cd5,\n\u6bd4\u5982\na.add b\n\u7b49\u4ef7\u4e8e\na.add(b)\n, \u6240\u4ee5\u8fd9\u91cc\u7684\u53c2\u6570\u5176\u5b9e\u662f\u4e00\u4e2aTuple: \n(vMethods, SampleToMiniBatch(...))\n.\n\n\n3.\u5728\u6bcf\u4e2a\u8282\u70b9\u6267\u884c\u4e00\u904d\u6a21\u578b\u7136\u540e\u6536\u96c6\u7ed3\u679c\n\n\n\u4ee3\u7801\u5c31\u662f\u8fd9\u4e00\u5806:\n\n\ndataset.mapPartitions(partition => {\n  val localModel = modelBroad.value()\n  val localMethod = otherBroad.value._1.map(_.clone())\n  val localTransformer = otherBroad.value._2.cloneTransformer()\n  val miniBatch = localTransformer(partition)\n  miniBatch.map(batch => {\n    val output = localModel.forward(batch.getInput())\n    localMethod.map(validation => {\n      validation(output, batch.getTarget())\n    })\n  })\n}).reduce((left, right) => {\n    left.zip(right).map { case (l, r) => l + r }\n}).zip(vMethods)\n\n\n\n\n\u5148\u662f\u6700\u9876\u5c42\u7684\nmapPartitions\n, \u7b80\u5355, spark\u7684\u673a\u5236\u662f\u4e00\u4e2a\u8282\u70b9\u4fdd\u5b58\u4e00\u4e2apartition, \u6240\u4ee5\u5462\u8fd9\u4e2a\u5c31\u662f\u5728\u6bcf\u4e2a\u8282\u70b9\u6267\u884c\u4e00\u904d\u540e\u9762\u7684\u90a3\u4e2a\u51fd\u6570\npartition=>{...}\n.\n\n\npartition\n\u8fd9\u4e2a\u53c2\u6570\u5c31\u662f\u4e00\u4e2a\u6570\u636e\u5206\u533a\u4e86.\n\n\n\u7ee7\u7eed\u770b\u51fd\u6570\u4f53, \u524d3\u53e5:\n\n\n  val localModel = modelBroad.value()\n  val localMethod = otherBroad.value._1.map(_.clone())\n  val localTransformer = otherBroad.value._2.cloneTransformer()\n\n\n\n\n\u524d\u9762\u8bf4\u4e86\u5728\u524d2\u6b65\u5e7f\u64ad\u4e86\u51e0\u4e2a\u53d8\u91cf, \u8fd9\u91cc\u5c31\u662f\u5728slave\u4e0a\u8bbf\u95ee\u90a3\u51e0\u4e2a\u53d8\u91cf, \nlocalModel\n\u662f\u6a21\u578b, \nlocalMethod\n\u662f\u90a3\u4e2a\u7edf\u8ba1\u65b9\u6cd5\u6570\u7ec4,\n\nlocalTransformer\n\u5c31\u662f\u628a\u6570\u636e\u8f6c\u6210\u4e00\u4e2a\u4e2abatch\u7684\u5bf9\u8c61.\n\n\n\u7136\u540e\u5c31\u662f\u8c03\u7528\u8fd9\u4e2a\nlocalTransformer\n\u5c06\u6570\u636e\u96c6\u8f6c\u6210batch.\n\n\n\u540e\u9762\u7684\u4ee3\u7801, \u9664\u4e86\u8fd9\u4e00\u53e5:\n\n\nscala\nval output = localModel.forward(batch.getInput())\n\n\n\u662f\u8fd0\u884c\u6a21\u578binference\u5916, \u5176\u4ed6\u90fd\u662f\u5728\u6536\u96c6\u7edf\u8ba1\u7ed3\u679c, \u53ef\u4ee5\u4e0d\u5fc5\u5173\u6ce8.\n\n\n\u6240\u4ee5\u6211\u4eec\u540e\u9762\u81f3\u4e8e\u5173\u6ce8\u6a21\u578b\u5982\u4f55forward.\n\n\n\u8fd9\u4e2a\u7559\u5728\u4e0b\u4e00\u8282 \nforward\n\u8be6\u8ff0.",
            "title": "inference\u8c03\u7528\u94fe\u5206\u6790"
        },
        {
            "location": "/BigDL/inference\u8c03\u7528\u94fe\u5206\u6790/#inference",
            "text": "\u4ee5\u6700\u7b80\u5355\u7684lenet5\u4e3a\u4f8b, \u63a2\u7a76inference\u8fc7\u7a0b\u7684\u8c03\u7528\u94fe  \u793a\u4f8b\u4ee3\u7801\u4f4d\u4e8e'spark/dl/src/main/scala/com/pzque/sparkdl/lenet', \u6a21\u578b\u7684checkpoint\u5df2\u4fdd\u5b58\u597d, \u4e0b\u8f7d\u597d\u6570\u636e\u540e\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c'Test.scala'\u67e5\u770b\u6d4b\u8bd5\u96c6\u4e0a\u7684\u63a8\u65ad\u51c6\u786e\u7387.",
            "title": "inference\u8c03\u7528\u94fe\u5206\u6790"
        },
        {
            "location": "/BigDL/inference\u8c03\u7528\u94fe\u5206\u6790/#lenet",
            "text": "\u9996\u5148\u770b\u4e00\u4e0blenet\u6a21\u578b\u7684\u5b9a\u4e49.  apply \u548c graph \u51fd\u6570\u5206\u522b\u4f7f\u7528\u4e86Sequential\u548cGraph\u7684API\u5b9a\u4e49\u6a21\u578b, \u4e8c\u8005\u662f\u7b49\u4ef7\u7684.  \u6a21\u578b\u7684\u7ed3\u6784\u975e\u5e38\u7b80\u5355, \u5728\u6d4b\u8bd5\u96c6\u4e0a\u53ef\u4ee5\u8fbe\u523098.93%\u7684\u51c6\u786e\u7387.  28*28 -> (Conv -> MaxPooling)*2 -> (FullConnected)*2 -> LogSoftMax  object LeNet5 {\n  def apply(classNum: Int): Module[Float] = {\n    val model = Sequential()\n    model.add(Reshape(Array(1, 28, 28)))\n      .add(SpatialConvolution(1, 6, 5, 5).setName(\"conv1_5x5\"))\n      .add(Tanh())\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Tanh())\n      .add(SpatialConvolution(6, 12, 5, 5).setName(\"conv2_5x5\"))\n      .add(SpatialMaxPooling(2, 2, 2, 2))\n      .add(Reshape(Array(12 * 4 * 4)))\n      .add(Linear(12 * 4 * 4, 100).setName(\"fc1\"))\n      .add(Tanh())\n      .add(Linear(100, classNum).setName(\"fc2\"))\n      .add(LogSoftMax())\n  }\n  def graph(classNum: Int): Module[Float] = {\n    val input = Reshape(Array(1, 28, 28)).inputs()\n    val conv1 = SpatialConvolution(1, 6, 5, 5).setName(\"conv1_5x5\").inputs(input)\n    val tanh1 = Tanh().inputs(conv1)\n    val pool1 = SpatialMaxPooling(2, 2, 2, 2).inputs(tanh1)\n    val tanh2 = Tanh().inputs(pool1)\n    val conv2 = SpatialConvolution(6, 12, 5, 5).setName(\"conv2_5x5\").inputs(tanh2)\n    val pool2 = SpatialMaxPooling(2, 2, 2, 2).inputs(conv2)\n    val reshape = Reshape(Array(12 * 4 * 4)).inputs(pool2)\n    val fc1 = Linear(12 * 4 * 4, 100).setName(\"fc1\").inputs(reshape)\n    val tanh3 = Tanh().inputs(fc1)\n    val fc2 = Linear(100, classNum).setName(\"fc2\").inputs(tanh3)\n    val output = LogSoftMax().inputs(fc2)\n\n    Graph(input, output)\n  }\n}",
            "title": "lenet\u6a21\u578b\u5b9a\u4e49"
        },
        {
            "location": "/BigDL/inference\u8c03\u7528\u94fe\u5206\u6790/#inference_1",
            "text": "infrence\u7684\u6838\u5fc3\u4ee3\u7801\u5982\u4e0b:         // \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e, \u8c03\u7528SparkContext\u7c7b\u7684parallize\u65b9\u6cd5\u5c06\u5176\u8f6c\u4e3aRDD\n      val rddData: RDD[ByteRecord] = sc.parallelize(load(validationData, validationLabel), partitionNum)\n\n      // \u5b9a\u4e49\u4e00\u4e2a\u6570\u636e\u9884\u5904\u7406\u5668, \u5c06ByteRecord\u683c\u5f0f\u8f6c\u4e3aSample[Float]\n      val transformer: Transformer[ByteRecord, Sample[Float]] =\n        BytesToGreyImg(28, 28) -> GreyImgNormalizer(testMean, testStd) -> GreyImgToSample()\n\n      // \u4f7f\u7528transformer\u6784\u9020\u9a8c\u8bc1\u96c6RDD\n      val evaluationSet: RDD[Sample[Float]] = transformer(rddData)\n\n      // \u52a0\u8f7d\u6a21\u578b\n      val model = Module.load[Float](param.model)\n\n      // \u6267\u884c\u6a21\u578b, \u83b7\u53d6\u7ed3\u679c\n      val result = model.evaluate(evaluationSet,\n        Array(new Top1Accuracy[Float]), Some(param.batchSize))  \u524d\u9762\u7684\u4e00\u5806\u90fd\u662f\u4f7f\u7528spark\u7684RDD\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\u4e0e\u8f6c\u6362, \u6700\u540e\u5f97\u5230 evaluationSet , \u4e5f\u662f\u4e00\u4e2aRDD, \u5143\u7d20\u662f Sample[Flaot] \u7684\u7c7b\u578b.  \u6211\u4eec\u770b\u5230\u5176\u5148\u662f\u901a\u8fc7 Module.load[Float] \u5c06\u6a21\u578b\u52a0\u8f7d\u8fdb\u6765, \u7136\u540e\u5229\u7528\u6a21\u578b\u6267\u884cevaluate\u64cd\u4f5c.  \u6211\u4eec\u9700\u8981\u5173\u6ce8\u8fd9\u4e00\u53e5:  model.evaluate(evaluationSet,\n        Array(new Top1Accuracy[Float]), \n        Some(param.batchSize))  \u627e\u5230\u5b83\u7684\u5b9a\u4e49, \u4f4d\u4e8e AbstractModule \u7c7b:    /**\n   * use ValidationMethod to evaluate module on the given rdd dataset\n   * @param dataset dataset for test\n   * @param vMethods validation methods\n   * @param batchSize total batchsize of all partitions,\n   *                  optional param and default 4 * partitionNum of dataset\n   * @return\n   */\n  final def evaluate(\n    dataset: RDD[Sample[T]],\n    vMethods: Array[ValidationMethod[T]],\n    batchSize: Option[Int] = None\n  ): Array[(ValidationResult, ValidationMethod[T])] = {\n    Evaluator(this).test(dataset, vMethods, batchSize)\n  }  \u4e09\u4e2a\u53c2\u6570,   dataset : \u662f\u4f60\u8981\u8fd0\u884c\u6a21\u578b\u7684\u6570\u636e\u96c6  vMethods : \u662f\u6700\u540e\u6a21\u578b\u8fd0\u884c\u5b8c\u6210\u8fd0\u884c\u7684\u4e00\u4e9b\u7edf\u8ba1\u5de5\u4f5c, \u6bd4\u5982\u8fd9\u91cc\u7684Top1Accuracy\u5c31\u662f\u7edf\u8ba1\u4e00\u4e0b\u51c6\u786e\u7387   batchSize : \u6ce8\u610f\u8fd9\u4e2a\u4e0d\u662f\u673a\u5668\u5b66\u4e60\u7684\u90a3\u4e2abatchsize(\u6bcf\u4e2abatch\u7684\u5927\u5c0f), \u800c\u662f\u5c06\u5168\u90e8\u7684\u6570\u636e\u96c6\u5206\u6210\u591a\u5c11batch   \u7136\u540e\u6700\u540e\u6267\u884c\u6a21\u578b\u7684\u4ee3\u7801\u5c31\u662f Evaluator(this).test(dataset, vMethods, batchSize) \u4e86, \u4e0b\u9762\u6765\u770b\u4e00\u4e0b\u5b83\u7684\u5b9e\u73b0.",
            "title": "inference\u8c03\u7528\u94fe"
        },
        {
            "location": "/BigDL/inference\u8c03\u7528\u94fe\u5206\u6790/#evaluator",
            "text": "```scala\n/* \n * model evaluator\n * @param model model to be evaluated\n  /\nclass Evaluator[T: ClassTag] private optim (implicit ev: TensorNumeric[T])\n  extends Serializable {  private val batchPerPartition = 4  /* \n   * Applies ValidationMethod to the model and rdd dataset.\n   * @param vMethods\n   * @param batchSize total batchsize\n   * @return\n    /\n  def test(dataset: RDD[Sample[T]],\n   vMethods: Array[ValidationMethod[T]],\n   batchSize: Option[Int] = None): Array[(ValidationResult, ValidationMethod[T])] = {  val modelBroad = ModelBroadcast[T]().broadcast(dataset.sparkContext, model.evaluate())\nval partitionNum = dataset.partitions.length\n\nval totalBatch = batchSize.getOrElse(batchPerPartition * partitionNum)\nval otherBroad = dataset.sparkContext.broadcast(vMethods, SampleToMiniBatch(\n  batchSize = totalBatch, partitionNum = Some(partitionNum)))\n\ndataset.mapPartitions(partition => {\n  val localModel = modelBroad.value()\n  val localMethod = otherBroad.value._1.map(_.clone())\n  val localTransformer = otherBroad.value._2.cloneTransformer()\n  val miniBatch = localTransformer(partition)\n  miniBatch.map(batch => {\n    val output = localModel.forward(batch.getInput())\n    localMethod.map(validation => {\n      validation(output, batch.getTarget())\n    })\n  })\n}).reduce((left, right) => {\n    left.zip(right).map { case (l, r) => l + r }\n}).zip(vMethods)  }\n}  \n\u4e0a\u9762\u662f\u8fd9\u4e2a\u7c7b\u7684\u5168\u90e8\u4ee3\u7801, \u8fd9\u4e2a\u7c7b\u4e5f\u53ea\u662f\u5728\u5168\u5c40\u505a\u8c03\u5ea6, \u5f88\u7b80\u5355. \u5177\u4f53\u7684\u6267\u884c\u903b\u8f91\u5f53\u7136\u8fd8\u662f\u5728`AbstractModule`\u7684\u5b9e\u73b0\u7c7b\u91cc\u5b9a\u4e49.\n\n\u5982\u4ee3\u7801\u6240\u793a, \u5728\u4e00\u4e2aRDD\u6570\u636e\u96c6\u4e0a\u6267\u884c\u6a21\u578b\u6709\u5982\u4e0b\u51e0\u6b65:\n\n**1.\u5c06\u6a21\u578b\u5e7f\u64ad\u5230\u5404\u4e2a\u8282\u70b9**\n\n```scala\nval modelBroad = ModelBroadcast[T]().broadcast(dataset.sparkContext, model.evaluate())  \u8fd9\u4e00\u53e5\u5c06\u6a21\u578b\u62f7\u8d1d\u5230\u4e86\u6bcf\u4e00\u4e2aspark\u8282\u70b9\u4e0a, \u8ba9\u5176\u90fd\u80fd\u8bbf\u95ee\u5230.  2.\u5c06vMethods\u548c\u4e00\u4e2a\u80fd\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4e00\u4e2a\u4e2abatch\u7684transformer\u5e7f\u64ad\u5230\u5404\u4e2a\u8282\u70b9  val otherBroad = dataset.sparkContext.broadcast\n(\n vMethods, \n SampleToMiniBatch(batchSize = totalBatch, partitionNum = Some(partitionNum))\n )  \u8fd9\u91cc\u6ce8\u610f\u4e00\u4e0b\u4e00\u4e2ascala\u8bed\u6cd5\u7684\u5751, \u4e8b\u5b9e\u4e0a broadcast \u51fd\u6570\u53ea\u80fd\u63a5\u53d7\u4e00\u4e2a\u53c2\u6570, \u4f46\u662fscala\u652f\u6301\u51fd\u6570\u4e0d\u5e26\u62ec\u53f7\u7684\u8c03\u7528\u8bed\u6cd5,\n\u6bd4\u5982 a.add b \u7b49\u4ef7\u4e8e a.add(b) , \u6240\u4ee5\u8fd9\u91cc\u7684\u53c2\u6570\u5176\u5b9e\u662f\u4e00\u4e2aTuple:  (vMethods, SampleToMiniBatch(...)) .  3.\u5728\u6bcf\u4e2a\u8282\u70b9\u6267\u884c\u4e00\u904d\u6a21\u578b\u7136\u540e\u6536\u96c6\u7ed3\u679c  \u4ee3\u7801\u5c31\u662f\u8fd9\u4e00\u5806:  dataset.mapPartitions(partition => {\n  val localModel = modelBroad.value()\n  val localMethod = otherBroad.value._1.map(_.clone())\n  val localTransformer = otherBroad.value._2.cloneTransformer()\n  val miniBatch = localTransformer(partition)\n  miniBatch.map(batch => {\n    val output = localModel.forward(batch.getInput())\n    localMethod.map(validation => {\n      validation(output, batch.getTarget())\n    })\n  })\n}).reduce((left, right) => {\n    left.zip(right).map { case (l, r) => l + r }\n}).zip(vMethods)  \u5148\u662f\u6700\u9876\u5c42\u7684 mapPartitions , \u7b80\u5355, spark\u7684\u673a\u5236\u662f\u4e00\u4e2a\u8282\u70b9\u4fdd\u5b58\u4e00\u4e2apartition, \u6240\u4ee5\u5462\u8fd9\u4e2a\u5c31\u662f\u5728\u6bcf\u4e2a\u8282\u70b9\u6267\u884c\u4e00\u904d\u540e\u9762\u7684\u90a3\u4e2a\u51fd\u6570 partition=>{...} .  partition \u8fd9\u4e2a\u53c2\u6570\u5c31\u662f\u4e00\u4e2a\u6570\u636e\u5206\u533a\u4e86.  \u7ee7\u7eed\u770b\u51fd\u6570\u4f53, \u524d3\u53e5:    val localModel = modelBroad.value()\n  val localMethod = otherBroad.value._1.map(_.clone())\n  val localTransformer = otherBroad.value._2.cloneTransformer()  \u524d\u9762\u8bf4\u4e86\u5728\u524d2\u6b65\u5e7f\u64ad\u4e86\u51e0\u4e2a\u53d8\u91cf, \u8fd9\u91cc\u5c31\u662f\u5728slave\u4e0a\u8bbf\u95ee\u90a3\u51e0\u4e2a\u53d8\u91cf,  localModel \u662f\u6a21\u578b,  localMethod \u662f\u90a3\u4e2a\u7edf\u8ba1\u65b9\u6cd5\u6570\u7ec4, localTransformer \u5c31\u662f\u628a\u6570\u636e\u8f6c\u6210\u4e00\u4e2a\u4e2abatch\u7684\u5bf9\u8c61.  \u7136\u540e\u5c31\u662f\u8c03\u7528\u8fd9\u4e2a localTransformer \u5c06\u6570\u636e\u96c6\u8f6c\u6210batch.  \u540e\u9762\u7684\u4ee3\u7801, \u9664\u4e86\u8fd9\u4e00\u53e5:  scala\nval output = localModel.forward(batch.getInput())  \u662f\u8fd0\u884c\u6a21\u578binference\u5916, \u5176\u4ed6\u90fd\u662f\u5728\u6536\u96c6\u7edf\u8ba1\u7ed3\u679c, \u53ef\u4ee5\u4e0d\u5fc5\u5173\u6ce8.  \u6240\u4ee5\u6211\u4eec\u540e\u9762\u81f3\u4e8e\u5173\u6ce8\u6a21\u578b\u5982\u4f55forward.  \u8fd9\u4e2a\u7559\u5728\u4e0b\u4e00\u8282  forward \u8be6\u8ff0.",
            "title": "Evaluator"
        },
        {
            "location": "/BigDL/forward/",
            "text": "",
            "title": "Forward"
        },
        {
            "location": "/BigDL/Tensor\u8fd0\u7b97/",
            "text": "",
            "title": "Tensor\u8fd0\u7b97"
        }
    ]
}