{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to SparkDL",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-sparkdl",
            "text": "",
            "title": "Welcome to SparkDL"
        },
        {
            "location": "/BigDL/\u6a21\u578b\u5b9a\u4e49/",
            "text": "BigDL\u7684\u4ee3\u7801\u67b6\u6784\u6a21\u4eff\u4e86Torch\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u6784\u5efa\u4ee5\nModule\n\u4e3a\u6838\u5fc3\uff0c\u5177\u4f53\u8fd0\u7b97\u5219\u4ee5\nTensor\n\u4e3a\u5355\u4f4d\u6267\u884c\u3002\n\n\n\u6a21\u578b\u5b9a\u4e49API\n\n\nBigDL\u652f\u63012\u5957\u6a21\u578b\u5b9a\u4e49API\n\n\n\n\nSequential\n\n\nFunctional\n\n\n\n\n\b\u5bf9\u4e00\u4e2a\u540c\u6837\u7684\u4f8b\u5b50\uff1a\n\n\nLinear -> Sigmoid -> Softmax\n\n\n\n\n\u91c7\u7528Sequential\u7684\u5f62\u5f0f\u5b9a\u4e49\u4e3a\uff1a\n\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())\n\n\n\n\nFunctional\u7684\u5f62\u5f0f\u4e3a\uff1a\n\n\nval linear = Linear(...).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = Softmax().inputs(sigmoid)\nval model = Graph(Seq[linear], Seq[softmax])\n\n\n\n\n\u540e\u8005\u7684\u5f62\u5f0f\u6bd4\u8f83\u76f4\u89c2\u3002",
            "title": "\u6a21\u578b\u5b9a\u4e49"
        },
        {
            "location": "/BigDL/\u6a21\u578b\u5b9a\u4e49/#api",
            "text": "BigDL\u652f\u63012\u5957\u6a21\u578b\u5b9a\u4e49API   Sequential  Functional   \b\u5bf9\u4e00\u4e2a\u540c\u6837\u7684\u4f8b\u5b50\uff1a  Linear -> Sigmoid -> Softmax  \u91c7\u7528Sequential\u7684\u5f62\u5f0f\u5b9a\u4e49\u4e3a\uff1a  val model = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())  Functional\u7684\u5f62\u5f0f\u4e3a\uff1a  val linear = Linear(...).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = Softmax().inputs(sigmoid)\nval model = Graph(Seq[linear], Seq[softmax])  \u540e\u8005\u7684\u5f62\u5f0f\u6bd4\u8f83\u76f4\u89c2\u3002",
            "title": "\u6a21\u578b\u5b9a\u4e49API"
        },
        {
            "location": "/BigDL/Module/",
            "text": "Module\n\n\nModule\n\u662fBigDL\u4e2d\b\u7f51\u7edc\u6784\u5efa\u7684\u57fa\u672c\u5355\u4f4d\uff0c\u7f51\u7edc\u7684\u6bcf\u4e00\u79cd\u5c42\u90fd\u5b9e\u73b0\u4e3a\u4e00\u4e2a\nModule\n\u3002\n\n\nAbstractModule\n\n\ncom.intel.analytics.bigdl.nn.abstractnn\n\u5305\u5185\u5b9a\u4e49\u4e86\nAbstractModule\n\uff0c\u5b83\u662f\u6240\u6709\nModule\n\u7684\u539f\u59cb\u57fa\u7c7b\uff1a\n\n\npackage com.intel.analytics.bigdl.nn.abstractnn\n/**\n * Module is the basic component of a neural network. It forward activities and backward gradients.\n * Modules can connect to others to construct a complex neural network.\n *\n * @tparam A Input data type\n * @tparam B Output data type\n * @tparam T The numeric type in this module parameters.\n */\nabstract class AbstractModule[A <: Activity: ClassTag, B <: Activity: ClassTag, T: ClassTag](\n  implicit ev: TensorNumeric[T]) extends Serializable with InferShape\n\n\n\n\n\u8fd9\u662f\u4e2a\u6cdb\u578b\u7c7b\uff0c\nAbstract[A,B,T]\n\uff0c\u67093\u4e2a\u53c2\u6570\nA,B,T\n\uff0c\nA\n\u662f\u8f93\u5165\u7684\u7c7b\u578b\uff0c\nB\n\u662f\u8f93\u51fa\u7684\u7c7b\u578b\uff0c\u90fd\u8981\u6c42\u662f\nActivity\n\u7684\u5b50\u7c7b\uff0c\u7136\u540e\nT\n\u662f\u6b64Module\u4f7f\u7528\u53c2\u6570\u7684\u7c7b\u578b\uff0c\u53ef\u4ee5\u662f\nDouble\n\u6216\u8005\nFloat\n.\n\n\n\u4e00\u4e2a\nModule\n\u7684\u6838\u5fc3\u529f\u80fd\u80af\u5b9a\u662f\u524d\u5411\u4f20\u64ad\u8fdb\u884c\u63a8\u65ad\u548c\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u68af\u5ea6, \u5206\u522b\u5bf9\u5e94\nforward\n\u548c\nbackward\n\u65b9\u6cd5, \u8fd9\u4e24\u4e2a\u65b9\u6cd5\u662f\u7ed9\u51fa\u5177\u4f53\u5b9e\u73b0\u4e86\u7684:\n\n\n  /**\n   * Takes an input object, and computes the corresponding output of the module. After a forward,\n   * the output state variable should have been updated to the new value.\n   *\n   * @param input input data\n   * @return output data\n   */\n  final def forward(input: A): B = {\n    val before = System.nanoTime()\n    try {\n      updateOutput(input)\n    } catch {\n      case l: LayerException =>\n        l.layerMsg = this.toString() + \"/\" + l.layerMsg\n        throw l\n      case e: Throwable =>\n        throw new LayerException(this.toString(), e)\n    }\n    forwardTime += System.nanoTime() - before\n\n    output\n  }\n\n  /**\n   * Performs a back-propagation step through the module, with respect to the given input. In\n   * general this method makes the assumption forward(input) has been called before, with the same\n   * input. This is necessary for optimization reasons. If you do not respect this rule, backward()\n   * will compute incorrect gradients.\n   *\n   * @param input input data\n   * @param gradOutput gradient of next layer\n   * @return gradient corresponding to input data\n   */\n  def backward(input: A, gradOutput: B): A = {\n    val before = System.nanoTime()\n    updateGradInput(input, gradOutput)\n    accGradParameters(input, gradOutput)\n    backwardTime += System.nanoTime() - before\n\n    gradInput\n  }\n\n\n\n\n\u6682\u65f6\u53ef\u4ee5\u53ea\u5173\u6ce8\nforward\n\u65b9\u6cd5, \u53ef\u4ee5\u53d1\u73b0\u5b83\u5c31\u662f\u8c03\u7528\u4e86\nupdateOutput(input)\n,\b \u7136\u540e\u505a\u4e00\u4e9b\u7edf\u8ba1\u5de5\u4f5c. \n\n\n  /**\n   * Computes the output using the current parameter set of the class and input. This function\n   * returns the result which is stored in the output field.\n   *\n   * @param input\n   * @return\n   */\n  def updateOutput(input: A): B\n\n\n\n\n\u8fd9\u662f\u4e2a\u62bd\u8c61\u65b9\u6cd5, \u7559\u7ed9\u5b50\u7c7b\u53bb\u5b9e\u73b0. \u6211\u4eec\u7ee7\u627f\u8fd9\u4e2a\u7c7b\u7136\u540e\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u5373\u53ef\u5b9e\u73b0\u524d\u5411\u4f20\u64ad\u8fdb\u884cinference\u4e86.\n\n\nTensorModule\n\n\n\b\u4e0a\u9762\u5b9a\u4e49\u4e86Module\u7684\u62bd\u8c61\u7c7b, \u7136\u540eBigDL\b\u5177\u4f53\u4f7f\u7528\u7684\u662f\u5b83\u7684\u4e00\u4e2a\u5b50\u7c7b\nTensorModule[T]\n, \u5206\u522b\u5c06\nAbstractModule\n\u7684\u4e09\u4e2a\u53c2\u6570\u7c7b\u578b\u8bbe\u4e3a\nTensor[T], Tensor[T], T\n, \u4e5f\u5c31\u662f\u8f93\u5165\u8f93\u51fa\u90fd\u662f\nTensor\n\u7c7b\u578b, \u8fd9\u6837\u5c31\u53ef\u4ee5\u628a\u5177\u4f53\u7684\u8ba1\u7b97\u8fc7\u7a0b\u5168\u90e8\u4f7f\u7528\nTensor\n\u7684\u8fd0\u7b97\u5b9e\u73b0.\n\n\n/**\n * [[TensorModule]] is an abstract sub-class of [[AbstractModule]], whose\n * input and output type both are [[Tensor]].\n *\n * @tparam T The numeric type in this module parameters\n */\nabstract class TensorModule[T: ClassTag]\n  (implicit ev: TensorNumeric[T]) extends AbstractModule[Tensor[T], Tensor[T], T]\n\n\n\n\n\u4f8b\u5b50\n\n\n\u770b\u4e00\u4e2a\u6700\u7b80\u5355\u7684\u4f8b\u5b50, \nAdd\n\u5c42, \u5b83\u7b80\u5355\u7684\u5c06\u6bcf\u4e2a\u8f93\u5165\u5404\u52a0\u4e0a\u4e00\u4e2a\u503c: \n\n\n/**\n * adds a bias term to input data ;\n *\n * @param inputSize size of input data\n */\n@SerialVersionUID(4268487849759172896L)\nclass Add[T: ClassTag](val inputSize: Int\n  )(implicit ev: TensorNumeric[T]) extends TensorModule[T] with Initializable {\n\n  val bias = Tensor[T](inputSize)\n\n  override def updateOutput(input: Tensor[T]): Tensor[T] = {\n    output.resizeAs(input).copy(input)\n    if (input.isSameSizeAs(bias)) {\n      output.add(bias)\n    } else {\n      val batchSize = input.size(1)\n      ones.resize(batchSize).fill(ev.one)\n      val biasLocal = bias.view(bias.size.product)\n      val outputLocal = output.view(batchSize, output.size.product/batchSize)\n      outputLocal.addr(ev.fromType[Int](1), ones, biasLocal)\n    }\n    output\n  }\n\n\n\n\noutput\n\u548c\nbias\n\u90fd\u662f\u4e00\u4e2a\nTensor\n\u7c7b\u7684\u5bf9\u8c61, \u5982\u679c\u4e8c\u8005\u7684\u5c3a\u5bf8\u76f8\u540c\u7684\u8bdd, \u76f4\u63a5\u8c03\u7528Tensor\u7684add\u65b9\u6cd5\n\n\noutput.add(bias)\n\n\n\n\n\u5c31\u53ef\u4ee5\u5f97\u5230\u8f93\u51fa.",
            "title": "Module"
        },
        {
            "location": "/BigDL/Module/#module",
            "text": "Module \u662fBigDL\u4e2d\b\u7f51\u7edc\u6784\u5efa\u7684\u57fa\u672c\u5355\u4f4d\uff0c\u7f51\u7edc\u7684\u6bcf\u4e00\u79cd\u5c42\u90fd\u5b9e\u73b0\u4e3a\u4e00\u4e2a Module \u3002",
            "title": "Module"
        },
        {
            "location": "/BigDL/Module/#abstractmodule",
            "text": "com.intel.analytics.bigdl.nn.abstractnn \u5305\u5185\u5b9a\u4e49\u4e86 AbstractModule \uff0c\u5b83\u662f\u6240\u6709 Module \u7684\u539f\u59cb\u57fa\u7c7b\uff1a  package com.intel.analytics.bigdl.nn.abstractnn\n/**\n * Module is the basic component of a neural network. It forward activities and backward gradients.\n * Modules can connect to others to construct a complex neural network.\n *\n * @tparam A Input data type\n * @tparam B Output data type\n * @tparam T The numeric type in this module parameters.\n */\nabstract class AbstractModule[A <: Activity: ClassTag, B <: Activity: ClassTag, T: ClassTag](\n  implicit ev: TensorNumeric[T]) extends Serializable with InferShape  \u8fd9\u662f\u4e2a\u6cdb\u578b\u7c7b\uff0c Abstract[A,B,T] \uff0c\u67093\u4e2a\u53c2\u6570 A,B,T \uff0c A \u662f\u8f93\u5165\u7684\u7c7b\u578b\uff0c B \u662f\u8f93\u51fa\u7684\u7c7b\u578b\uff0c\u90fd\u8981\u6c42\u662f Activity \u7684\u5b50\u7c7b\uff0c\u7136\u540e T \u662f\u6b64Module\u4f7f\u7528\u53c2\u6570\u7684\u7c7b\u578b\uff0c\u53ef\u4ee5\u662f Double \u6216\u8005 Float .  \u4e00\u4e2a Module \u7684\u6838\u5fc3\u529f\u80fd\u80af\u5b9a\u662f\u524d\u5411\u4f20\u64ad\u8fdb\u884c\u63a8\u65ad\u548c\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u68af\u5ea6, \u5206\u522b\u5bf9\u5e94 forward \u548c backward \u65b9\u6cd5, \u8fd9\u4e24\u4e2a\u65b9\u6cd5\u662f\u7ed9\u51fa\u5177\u4f53\u5b9e\u73b0\u4e86\u7684:    /**\n   * Takes an input object, and computes the corresponding output of the module. After a forward,\n   * the output state variable should have been updated to the new value.\n   *\n   * @param input input data\n   * @return output data\n   */\n  final def forward(input: A): B = {\n    val before = System.nanoTime()\n    try {\n      updateOutput(input)\n    } catch {\n      case l: LayerException =>\n        l.layerMsg = this.toString() + \"/\" + l.layerMsg\n        throw l\n      case e: Throwable =>\n        throw new LayerException(this.toString(), e)\n    }\n    forwardTime += System.nanoTime() - before\n\n    output\n  }\n\n  /**\n   * Performs a back-propagation step through the module, with respect to the given input. In\n   * general this method makes the assumption forward(input) has been called before, with the same\n   * input. This is necessary for optimization reasons. If you do not respect this rule, backward()\n   * will compute incorrect gradients.\n   *\n   * @param input input data\n   * @param gradOutput gradient of next layer\n   * @return gradient corresponding to input data\n   */\n  def backward(input: A, gradOutput: B): A = {\n    val before = System.nanoTime()\n    updateGradInput(input, gradOutput)\n    accGradParameters(input, gradOutput)\n    backwardTime += System.nanoTime() - before\n\n    gradInput\n  }  \u6682\u65f6\u53ef\u4ee5\u53ea\u5173\u6ce8 forward \u65b9\u6cd5, \u53ef\u4ee5\u53d1\u73b0\u5b83\u5c31\u662f\u8c03\u7528\u4e86 updateOutput(input) ,\b \u7136\u540e\u505a\u4e00\u4e9b\u7edf\u8ba1\u5de5\u4f5c.     /**\n   * Computes the output using the current parameter set of the class and input. This function\n   * returns the result which is stored in the output field.\n   *\n   * @param input\n   * @return\n   */\n  def updateOutput(input: A): B  \u8fd9\u662f\u4e2a\u62bd\u8c61\u65b9\u6cd5, \u7559\u7ed9\u5b50\u7c7b\u53bb\u5b9e\u73b0. \u6211\u4eec\u7ee7\u627f\u8fd9\u4e2a\u7c7b\u7136\u540e\u5b9e\u73b0\u8fd9\u4e2a\u65b9\u6cd5\u5373\u53ef\u5b9e\u73b0\u524d\u5411\u4f20\u64ad\u8fdb\u884cinference\u4e86.",
            "title": "AbstractModule"
        },
        {
            "location": "/BigDL/Module/#tensormodule",
            "text": "\b\u4e0a\u9762\u5b9a\u4e49\u4e86Module\u7684\u62bd\u8c61\u7c7b, \u7136\u540eBigDL\b\u5177\u4f53\u4f7f\u7528\u7684\u662f\u5b83\u7684\u4e00\u4e2a\u5b50\u7c7b TensorModule[T] , \u5206\u522b\u5c06 AbstractModule \u7684\u4e09\u4e2a\u53c2\u6570\u7c7b\u578b\u8bbe\u4e3a Tensor[T], Tensor[T], T , \u4e5f\u5c31\u662f\u8f93\u5165\u8f93\u51fa\u90fd\u662f Tensor \u7c7b\u578b, \u8fd9\u6837\u5c31\u53ef\u4ee5\u628a\u5177\u4f53\u7684\u8ba1\u7b97\u8fc7\u7a0b\u5168\u90e8\u4f7f\u7528 Tensor \u7684\u8fd0\u7b97\u5b9e\u73b0.  /**\n * [[TensorModule]] is an abstract sub-class of [[AbstractModule]], whose\n * input and output type both are [[Tensor]].\n *\n * @tparam T The numeric type in this module parameters\n */\nabstract class TensorModule[T: ClassTag]\n  (implicit ev: TensorNumeric[T]) extends AbstractModule[Tensor[T], Tensor[T], T]",
            "title": "TensorModule"
        },
        {
            "location": "/BigDL/Module/#_1",
            "text": "\u770b\u4e00\u4e2a\u6700\u7b80\u5355\u7684\u4f8b\u5b50,  Add \u5c42, \u5b83\u7b80\u5355\u7684\u5c06\u6bcf\u4e2a\u8f93\u5165\u5404\u52a0\u4e0a\u4e00\u4e2a\u503c:   /**\n * adds a bias term to input data ;\n *\n * @param inputSize size of input data\n */\n@SerialVersionUID(4268487849759172896L)\nclass Add[T: ClassTag](val inputSize: Int\n  )(implicit ev: TensorNumeric[T]) extends TensorModule[T] with Initializable {\n\n  val bias = Tensor[T](inputSize)\n\n  override def updateOutput(input: Tensor[T]): Tensor[T] = {\n    output.resizeAs(input).copy(input)\n    if (input.isSameSizeAs(bias)) {\n      output.add(bias)\n    } else {\n      val batchSize = input.size(1)\n      ones.resize(batchSize).fill(ev.one)\n      val biasLocal = bias.view(bias.size.product)\n      val outputLocal = output.view(batchSize, output.size.product/batchSize)\n      outputLocal.addr(ev.fromType[Int](1), ones, biasLocal)\n    }\n    output\n  }  output \u548c bias \u90fd\u662f\u4e00\u4e2a Tensor \u7c7b\u7684\u5bf9\u8c61, \u5982\u679c\u4e8c\u8005\u7684\u5c3a\u5bf8\u76f8\u540c\u7684\u8bdd, \u76f4\u63a5\u8c03\u7528Tensor\u7684add\u65b9\u6cd5  output.add(bias)  \u5c31\u53ef\u4ee5\u5f97\u5230\u8f93\u51fa.",
            "title": "\u4f8b\u5b50"
        },
        {
            "location": "/BigDL/Graph/",
            "text": "",
            "title": "Graph"
        }
    ]
}