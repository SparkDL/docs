{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to MkDocs\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-mkdocs",
            "text": "For full documentation visit  mkdocs.org .",
            "title": "Welcome to MkDocs"
        },
        {
            "location": "/#commands",
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.",
            "title": "Commands"
        },
        {
            "location": "/#project-layout",
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.",
            "title": "Project layout"
        },
        {
            "location": "/BigDL/\u6574\u4f53\u67b6\u6784/",
            "text": "BigDL\u7684\u4ee3\u7801\u67b6\u6784\u6a21\u4eff\u4e86Torch\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u6784\u5efa\u4ee5\nModule\n\u4e3a\u6838\u5fc3\uff0c\u5177\u4f53\u8fd0\u7b97\u5219\u4ee5\nTensor\n\u4e3a\u5355\u4f4d\u6267\u884c\u3002\n\n\n\u6a21\u578b\u5b9a\u4e49API\n\n\nBigDL\u652f\u63012\u5957\u6a21\u578b\u5b9a\u4e49API\n\n\n\n\nSequential\n\n\nFunctional\n\n\n\n\n\b\u5bf9\u4e00\u4e2a\u540c\u6837\u7684\u4f8b\u5b50\uff1a\n\n\nLinear -> Sigmoid -> Softmax\n\n\n\n\n\u91c7\u7528Sequential\u7684\u5f62\u5f0f\u5b9a\u4e49\u4e3a\uff1a\n\n\nval model = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())\n\n\n\n\nFunctional\u7684\u5f62\u5f0f\u4e3a\uff1a\n\n\nval linear = Linear(...).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = Softmax().inputs(sigmoid)\nval model = Graph(Seq[linear], Seq[softmax])\n\n\n\n\n\u540e\u8005\u7684\u5f62\u5f0f\u6bd4\u8f83\u76f4\u89c2\u3002\n\n\nModule\n\n\nModule\n\u662fBigDL\u4e2d\b\u7f51\u7edc\u6784\u5efa\u7684\u57fa\u672c\u5355\u4f4d\uff0c\u7f51\u7edc\u7684\u6bcf\u4e00\u79cd\u5c42\u90fd\u5b9e\u73b0\u4e3a\u4e00\u4e2a\nModule\n\u3002\n\n\ncom.intel.analytics.bigdl.nn.abstractnn\n\u5305\u5185\u5b9a\u4e49\u4e86\nAbstractModule\n\uff0c\u5b83\u662f\u6240\u6709\nModule\n\u7684\u539f\u59cb\u57fa\u7c7b\uff1a\n\n\npackage com.intel.analytics.bigdl.nn.abstractnn\n/**\n * Module is the basic component of a neural network. It forward activities and backward gradients.\n * Modules can connect to others to construct a complex neural network.\n *\n * @tparam A Input data type\n * @tparam B Output data type\n * @tparam T The numeric type in this module parameters.\n */\nabstract class AbstractModule[A <: Activity: ClassTag, B <: Activity: ClassTag, T: ClassTag](\n  implicit ev: TensorNumeric[T]) extends Serializable with InferShape\n\n\n\n\n\u8fd9\u662f\u4e2a\u6cdb\u578b\u7c7b\uff0c\nAbstract[A,B,T]\n\uff0c\u67093\u4e2a\u53c2\u6570\nA,B,T\n\uff0c\nA\n\u662f\u8f93\u5165\u7684\u7c7b\u578b\uff0c\nB\n\u662f\u8f93\u51fa\u7684\u7c7b\u578b\uff0c\u90fd\u8981\u6c42\u662f\nActivity\n\u7684\u5b50\u7c7b\uff0c\u7136\u540e\nT\n\u662f\u6b64Module\u4f7f\u7528\u53c2\u6570\u7684\u7c7b\u578b\uff0c\u53ef\u4ee5\u662f\nDouble\n\u6216\u8005\nFloat\n.\n\n\n\u4e00\u4e2a\nModule\n\u7684\u6838\u5fc3\u529f\u80fd\u80af\u5b9a\u662f\u524d\u5411\u4f20\u64ad\u8fdb\u884c\u63a8\u65ad\u548c\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u68af\u5ea6, \u5206\u522b\u5bf9\u5e94\nforward\n\u548c\nbackward\n\u65b9\u6cd5, \u8fd9\u4e24\u4e2a\u65b9\u6cd5\u662f\u7ed9\u51fa\u5177\u4f53\u5b9e\u73b0\u4e86\u7684:\n\n\n  /**\n   * Takes an input object, and computes the corresponding output of the module. After a forward,\n   * the output state variable should have been updated to the new value.\n   *\n   * @param input input data\n   * @return output data\n   */\n  final def forward(input: A): B = {\n    val before = System.nanoTime()\n    try {\n      updateOutput(input)\n    } catch {\n      case l: LayerException =>\n        l.layerMsg = this.toString() + \"/\" + l.layerMsg\n        throw l\n      case e: Throwable =>\n        throw new LayerException(this.toString(), e)\n    }\n    forwardTime += System.nanoTime() - before\n\n    output\n  }\n\n  /**\n   * Performs a back-propagation step through the module, with respect to the given input. In\n   * general this method makes the assumption forward(input) has been called before, with the same\n   * input. This is necessary for optimization reasons. If you do not respect this rule, backward()\n   * will compute incorrect gradients.\n   *\n   * @param input input data\n   * @param gradOutput gradient of next layer\n   * @return gradient corresponding to input data\n   */\n  def backward(input: A, gradOutput: B): A = {\n    val before = System.nanoTime()\n    updateGradInput(input, gradOutput)\n    accGradParameters(input, gradOutput)\n    backwardTime += System.nanoTime() - before\n\n    gradInput\n  }\n\n\n\n\n\u6682\u65f6\u53ef\u4ee5\u53ea\u5173\u6ce8\nforward\n\u65b9\u6cd5, \u53ef\u4ee5\u53d1\u73b0\u5b83\u5c31\u662f\u8c03\u7528\u4e86\nupdateOutput(input)\n,\b \u7136\u540e\u505a\u4e00\u4e9b\u7edf\u8ba1\u5de5\u4f5c. \n\n\n  /**\n   * Computes the output using the current parameter set of the class and input. This function\n   * returns the result which is stored in the output field.\n   *\n   * @param input\n   * @return\n   */\n  def updateOutput(input: A): B",
            "title": "\u6574\u4f53\u67b6\u6784"
        },
        {
            "location": "/BigDL/\u6574\u4f53\u67b6\u6784/#api",
            "text": "BigDL\u652f\u63012\u5957\u6a21\u578b\u5b9a\u4e49API   Sequential  Functional   \b\u5bf9\u4e00\u4e2a\u540c\u6837\u7684\u4f8b\u5b50\uff1a  Linear -> Sigmoid -> Softmax  \u91c7\u7528Sequential\u7684\u5f62\u5f0f\u5b9a\u4e49\u4e3a\uff1a  val model = Sequential()\nmodel.add(Linear(...))\nmodel.add(Sigmoid())\nmodel.add(Softmax())  Functional\u7684\u5f62\u5f0f\u4e3a\uff1a  val linear = Linear(...).inputs()\nval sigmoid = Sigmoid().inputs(linear)\nval softmax = Softmax().inputs(sigmoid)\nval model = Graph(Seq[linear], Seq[softmax])  \u540e\u8005\u7684\u5f62\u5f0f\u6bd4\u8f83\u76f4\u89c2\u3002",
            "title": "\u6a21\u578b\u5b9a\u4e49API"
        },
        {
            "location": "/BigDL/\u6574\u4f53\u67b6\u6784/#module",
            "text": "Module \u662fBigDL\u4e2d\b\u7f51\u7edc\u6784\u5efa\u7684\u57fa\u672c\u5355\u4f4d\uff0c\u7f51\u7edc\u7684\u6bcf\u4e00\u79cd\u5c42\u90fd\u5b9e\u73b0\u4e3a\u4e00\u4e2a Module \u3002  com.intel.analytics.bigdl.nn.abstractnn \u5305\u5185\u5b9a\u4e49\u4e86 AbstractModule \uff0c\u5b83\u662f\u6240\u6709 Module \u7684\u539f\u59cb\u57fa\u7c7b\uff1a  package com.intel.analytics.bigdl.nn.abstractnn\n/**\n * Module is the basic component of a neural network. It forward activities and backward gradients.\n * Modules can connect to others to construct a complex neural network.\n *\n * @tparam A Input data type\n * @tparam B Output data type\n * @tparam T The numeric type in this module parameters.\n */\nabstract class AbstractModule[A <: Activity: ClassTag, B <: Activity: ClassTag, T: ClassTag](\n  implicit ev: TensorNumeric[T]) extends Serializable with InferShape  \u8fd9\u662f\u4e2a\u6cdb\u578b\u7c7b\uff0c Abstract[A,B,T] \uff0c\u67093\u4e2a\u53c2\u6570 A,B,T \uff0c A \u662f\u8f93\u5165\u7684\u7c7b\u578b\uff0c B \u662f\u8f93\u51fa\u7684\u7c7b\u578b\uff0c\u90fd\u8981\u6c42\u662f Activity \u7684\u5b50\u7c7b\uff0c\u7136\u540e T \u662f\u6b64Module\u4f7f\u7528\u53c2\u6570\u7684\u7c7b\u578b\uff0c\u53ef\u4ee5\u662f Double \u6216\u8005 Float .  \u4e00\u4e2a Module \u7684\u6838\u5fc3\u529f\u80fd\u80af\u5b9a\u662f\u524d\u5411\u4f20\u64ad\u8fdb\u884c\u63a8\u65ad\u548c\u53cd\u5411\u4f20\u64ad\u66f4\u65b0\u68af\u5ea6, \u5206\u522b\u5bf9\u5e94 forward \u548c backward \u65b9\u6cd5, \u8fd9\u4e24\u4e2a\u65b9\u6cd5\u662f\u7ed9\u51fa\u5177\u4f53\u5b9e\u73b0\u4e86\u7684:    /**\n   * Takes an input object, and computes the corresponding output of the module. After a forward,\n   * the output state variable should have been updated to the new value.\n   *\n   * @param input input data\n   * @return output data\n   */\n  final def forward(input: A): B = {\n    val before = System.nanoTime()\n    try {\n      updateOutput(input)\n    } catch {\n      case l: LayerException =>\n        l.layerMsg = this.toString() + \"/\" + l.layerMsg\n        throw l\n      case e: Throwable =>\n        throw new LayerException(this.toString(), e)\n    }\n    forwardTime += System.nanoTime() - before\n\n    output\n  }\n\n  /**\n   * Performs a back-propagation step through the module, with respect to the given input. In\n   * general this method makes the assumption forward(input) has been called before, with the same\n   * input. This is necessary for optimization reasons. If you do not respect this rule, backward()\n   * will compute incorrect gradients.\n   *\n   * @param input input data\n   * @param gradOutput gradient of next layer\n   * @return gradient corresponding to input data\n   */\n  def backward(input: A, gradOutput: B): A = {\n    val before = System.nanoTime()\n    updateGradInput(input, gradOutput)\n    accGradParameters(input, gradOutput)\n    backwardTime += System.nanoTime() - before\n\n    gradInput\n  }  \u6682\u65f6\u53ef\u4ee5\u53ea\u5173\u6ce8 forward \u65b9\u6cd5, \u53ef\u4ee5\u53d1\u73b0\u5b83\u5c31\u662f\u8c03\u7528\u4e86 updateOutput(input) ,\b \u7136\u540e\u505a\u4e00\u4e9b\u7edf\u8ba1\u5de5\u4f5c.     /**\n   * Computes the output using the current parameter set of the class and input. This function\n   * returns the result which is stored in the output field.\n   *\n   * @param input\n   * @return\n   */\n  def updateOutput(input: A): B",
            "title": "Module"
        }
    ]
}