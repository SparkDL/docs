<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Module - SparkDL</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Module", url: "#module", children: [
              {title: "AbstractModule", url: "#abstractmodule" },
              {title: "TensorModule", url: "#tensormodule" },
              {title: "\u4f8b\u5b50", url: "#_1" },
          ]},
        ];

    </script>
    <script src="../../js/base.js"></script>
      <script src="../../search/require.js"></script>
      <script src="../../search/search.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    

    <h1 id="module">Module</h1>
<p><code>Module</code>是BigDL中网络构建的基本单位，网络的每一种层都实现为一个<code>Module</code>。</p>
<h2 id="abstractmodule">AbstractModule</h2>
<p><code>com.intel.analytics.bigdl.nn.abstractnn</code>包内定义了<code>AbstractModule</code>，它是所有<code>Module</code>的原始基类：</p>
<pre><code class="scala">package com.intel.analytics.bigdl.nn.abstractnn
/**
 * Module is the basic component of a neural network. It forward activities and backward gradients.
 * Modules can connect to others to construct a complex neural network.
 *
 * @tparam A Input data type
 * @tparam B Output data type
 * @tparam T The numeric type in this module parameters.
 */
abstract class AbstractModule[A &lt;: Activity: ClassTag, B &lt;: Activity: ClassTag, T: ClassTag](
  implicit ev: TensorNumeric[T]) extends Serializable with InferShape
</code></pre>

<p>这是个泛型类，<code>Abstract[A,B,T]</code>，有3个参数<code>A,B,T</code>，<code>A</code>是输入的类型，<code>B</code>是输出的类型，都要求是<code>Activity</code>的子类，然后<code>T</code>是此Module使用参数的类型，可以是<code>Double</code>或者<code>Float</code>.</p>
<p>一个<code>Module</code>的核心功能肯定是前向传播进行推断和反向传播更新梯度, 分别对应<code>forward</code>和<code>backward</code>方法, 这两个方法是给出具体实现了的:</p>
<pre><code class="scala">  /**
   * Takes an input object, and computes the corresponding output of the module. After a forward,
   * the output state variable should have been updated to the new value.
   *
   * @param input input data
   * @return output data
   */
  final def forward(input: A): B = {
    val before = System.nanoTime()
    try {
      updateOutput(input)
    } catch {
      case l: LayerException =&gt;
        l.layerMsg = this.toString() + &quot;/&quot; + l.layerMsg
        throw l
      case e: Throwable =&gt;
        throw new LayerException(this.toString(), e)
    }
    forwardTime += System.nanoTime() - before

    output
  }

  /**
   * Performs a back-propagation step through the module, with respect to the given input. In
   * general this method makes the assumption forward(input) has been called before, with the same
   * input. This is necessary for optimization reasons. If you do not respect this rule, backward()
   * will compute incorrect gradients.
   *
   * @param input input data
   * @param gradOutput gradient of next layer
   * @return gradient corresponding to input data
   */
  def backward(input: A, gradOutput: B): A = {
    val before = System.nanoTime()
    updateGradInput(input, gradOutput)
    accGradParameters(input, gradOutput)
    backwardTime += System.nanoTime() - before

    gradInput
  }
</code></pre>

<p>暂时可以只关注<code>forward</code>方法, 可以发现它就是调用了<code>updateOutput(input)</code>, 然后做一些统计工作. </p>
<pre><code class="scala">  /**
   * Computes the output using the current parameter set of the class and input. This function
   * returns the result which is stored in the output field.
   *
   * @param input
   * @return
   */
  def updateOutput(input: A): B
</code></pre>

<p>这是个抽象方法, 留给子类去实现. 我们继承这个类然后实现这个方法即可实现前向传播进行inference了.</p>
<h2 id="tensormodule">TensorModule</h2>
<p>上面定义了Module的抽象类, 然后BigDL具体使用的是它的一个子类<code>TensorModule[T]</code>, 分别将<code>AbstractModule</code>的三个参数类型设为<code>Tensor[T], Tensor[T], T</code>, 也就是输入输出都是<code>Tensor</code>类型, 这样就可以把具体的计算过程全部使用<code>Tensor</code>的运算实现.</p>
<pre><code class="scala">/**
 * [[TensorModule]] is an abstract sub-class of [[AbstractModule]], whose
 * input and output type both are [[Tensor]].
 *
 * @tparam T The numeric type in this module parameters
 */
abstract class TensorModule[T: ClassTag]
  (implicit ev: TensorNumeric[T]) extends AbstractModule[Tensor[T], Tensor[T], T]
</code></pre>

<h2 id="_1">例子</h2>
<p>看一个最简单的例子, <code>Add</code>层, 它简单的将每个输入各加上一个值: </p>
<pre><code class="scala">/**
 * adds a bias term to input data ;
 *
 * @param inputSize size of input data
 */
@SerialVersionUID(4268487849759172896L)
class Add[T: ClassTag](val inputSize: Int
  )(implicit ev: TensorNumeric[T]) extends TensorModule[T] with Initializable {

  val bias = Tensor[T](inputSize)

  override def updateOutput(input: Tensor[T]): Tensor[T] = {
    output.resizeAs(input).copy(input)
    if (input.isSameSizeAs(bias)) {
      output.add(bias)
    } else {
      val batchSize = input.size(1)
      ones.resize(batchSize).fill(ev.one)
      val biasLocal = bias.view(bias.size.product)
      val outputLocal = output.view(batchSize, output.size.product/batchSize)
      outputLocal.addr(ev.fromType[Int](1), ones, biasLocal)
    }
    output
  }
</code></pre>

<p><code>output</code>和<code>bias</code>都是一个<code>Tensor</code>类的对象, 如果二者的尺寸相同的话, 直接调用Tensor的add方法</p>
<pre><code class="scala">output.add(bias)
</code></pre>

<p>就可以得到输出.</p>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>