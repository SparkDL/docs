<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>inference调用链分析 - SparkDL</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "inference\u8c03\u7528\u94fe\u5206\u6790", url: "#inference", children: [
              {title: "lenet\u6a21\u578b\u5b9a\u4e49", url: "#lenet" },
              {title: "inference\u8c03\u7528\u94fe", url: "#inference_1" },
              {title: "Evaluator", url: "#evaluator" },
          ]},
        ];

    </script>
    <script src="../../js/base.js"></script>
      <script src="../../search/require.js"></script>
      <script src="../../search/search.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    

    <h1 id="inference">inference调用链分析</h1>
<p>以最简单的lenet5为例, 探究inference过程的调用链</p>
<p>示例代码位于'spark/dl/src/main/scala/com/pzque/sparkdl/lenet', 模型的checkpoint已保存好, 下载好数据后可以直接运行'Test.scala'查看测试集上的推断准确率.</p>
<h2 id="lenet">lenet模型定义</h2>
<p>首先看一下lenet模型的定义.</p>
<p><code>apply</code>和<code>graph</code>函数分别使用了Sequential和Graph的API定义模型, 二者是等价的.</p>
<p>模型的结构非常简单, 在测试集上可以达到98.93%的准确率.</p>
<pre><code class="scala">28*28 -&gt; (Conv -&gt; MaxPooling)*2 -&gt; (FullConnected)*2 -&gt; LogSoftMax
</code></pre>

<pre><code class="scala">object LeNet5 {
  def apply(classNum: Int): Module[Float] = {
    val model = Sequential()
    model.add(Reshape(Array(1, 28, 28)))
      .add(SpatialConvolution(1, 6, 5, 5).setName(&quot;conv1_5x5&quot;))
      .add(Tanh())
      .add(SpatialMaxPooling(2, 2, 2, 2))
      .add(Tanh())
      .add(SpatialConvolution(6, 12, 5, 5).setName(&quot;conv2_5x5&quot;))
      .add(SpatialMaxPooling(2, 2, 2, 2))
      .add(Reshape(Array(12 * 4 * 4)))
      .add(Linear(12 * 4 * 4, 100).setName(&quot;fc1&quot;))
      .add(Tanh())
      .add(Linear(100, classNum).setName(&quot;fc2&quot;))
      .add(LogSoftMax())
  }
  def graph(classNum: Int): Module[Float] = {
    val input = Reshape(Array(1, 28, 28)).inputs()
    val conv1 = SpatialConvolution(1, 6, 5, 5).setName(&quot;conv1_5x5&quot;).inputs(input)
    val tanh1 = Tanh().inputs(conv1)
    val pool1 = SpatialMaxPooling(2, 2, 2, 2).inputs(tanh1)
    val tanh2 = Tanh().inputs(pool1)
    val conv2 = SpatialConvolution(6, 12, 5, 5).setName(&quot;conv2_5x5&quot;).inputs(tanh2)
    val pool2 = SpatialMaxPooling(2, 2, 2, 2).inputs(conv2)
    val reshape = Reshape(Array(12 * 4 * 4)).inputs(pool2)
    val fc1 = Linear(12 * 4 * 4, 100).setName(&quot;fc1&quot;).inputs(reshape)
    val tanh3 = Tanh().inputs(fc1)
    val fc2 = Linear(100, classNum).setName(&quot;fc2&quot;).inputs(tanh3)
    val output = LogSoftMax().inputs(fc2)

    Graph(input, output)
  }
}
</code></pre>

<h2 id="inference_1">inference调用链</h2>
<p>infrence的核心代码如下: </p>
<pre><code class="scala">      // 加载测试数据, 调用SparkContext类的parallize方法将其转为RDD
      val rddData: RDD[ByteRecord] = sc.parallelize(load(validationData, validationLabel), partitionNum)

      // 定义一个数据预处理器, 将ByteRecord格式转为Sample[Float]
      val transformer: Transformer[ByteRecord, Sample[Float]] =
        BytesToGreyImg(28, 28) -&gt; GreyImgNormalizer(testMean, testStd) -&gt; GreyImgToSample()

      // 使用transformer构造验证集RDD
      val evaluationSet: RDD[Sample[Float]] = transformer(rddData)

      // 加载模型
      val model = Module.load[Float](param.model)

      // 执行模型, 获取结果
      val result = model.evaluate(evaluationSet,
        Array(new Top1Accuracy[Float]), Some(param.batchSize))
</code></pre>

<p>前面的一堆都是使用spark的RDD进行数据预处理与转换, 最后得到<code>evaluationSet</code>, 也是一个RDD, 元素是<code>Sample[Flaot]</code>的类型.</p>
<p>我们需要关注这一句:</p>
<pre><code class="scala">model.evaluate(evaluationSet,
        Array(new Top1Accuracy[Float]), 
        Some(param.batchSize))
</code></pre>

<p>找到它的定义:</p>
<pre><code class="scala">  /**
   * use ValidationMethod to evaluate module on the given rdd dataset
   * @param dataset dataset for test
   * @param vMethods validation methods
   * @param batchSize total batchsize of all partitions,
   *                  optional param and default 4 * partitionNum of dataset
   * @return
   */
  final def evaluate(
    dataset: RDD[Sample[T]],
    vMethods: Array[ValidationMethod[T]],
    batchSize: Option[Int] = None
  ): Array[(ValidationResult, ValidationMethod[T])] = {
    Evaluator(this).test(dataset, vMethods, batchSize)
  }
</code></pre>

<p>三个参数,</p>
<ul>
<li><code>dataset</code>: 是你要运行模型的数据集</li>
<li><code>vMethods</code>: 是最后模型运行完成运行的一些统计工作, 比如这里的Top1Accuracy就是统计一下准确率 </li>
<li><code>batchSize</code>: 注意这个不是机器学习的那个batchsize(每个batch的大小), 而是将全部的数据集分成多少batch</li>
</ul>
<p>然后最后执行模型的代码就是<code>Evaluator(this).test(dataset, vMethods, batchSize)</code>了, 下面来看一下它的实现.</p>
<h2 id="evaluator">Evaluator</h2>
<p>```scala
/*<em>
 * model evaluator
 * @param model model to be evaluated
 </em>/
class Evaluator[T: ClassTag] private<a href="model: Module[T]">optim</a>(implicit ev: TensorNumeric[T])
  extends Serializable {</p>
<p>private val batchPerPartition = 4</p>
<p>/*<em>
   * Applies ValidationMethod to the model and rdd dataset.
   * @param vMethods
   * @param batchSize total batchsize
   * @return
   </em>/
  def test(dataset: RDD[Sample[T]],
   vMethods: Array[ValidationMethod[T]],
   batchSize: Option[Int] = None): Array[(ValidationResult, ValidationMethod[T])] = {</p>
<pre><code>val modelBroad = ModelBroadcast[T]().broadcast(dataset.sparkContext, model.evaluate())
val partitionNum = dataset.partitions.length

val totalBatch = batchSize.getOrElse(batchPerPartition * partitionNum)
val otherBroad = dataset.sparkContext.broadcast(vMethods, SampleToMiniBatch(
  batchSize = totalBatch, partitionNum = Some(partitionNum)))

dataset.mapPartitions(partition =&gt; {
  val localModel = modelBroad.value()
  val localMethod = otherBroad.value._1.map(_.clone())
  val localTransformer = otherBroad.value._2.cloneTransformer()
  val miniBatch = localTransformer(partition)
  miniBatch.map(batch =&gt; {
    val output = localModel.forward(batch.getInput())
    localMethod.map(validation =&gt; {
      validation(output, batch.getTarget())
    })
  })
}).reduce((left, right) =&gt; {
    left.zip(right).map { case (l, r) =&gt; l + r }
}).zip(vMethods)
</code></pre>
<p>}
}
```</p>
<p>上面是这个类的全部代码</p>

  <br>
</div>

<footer class="col-md-12 wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>